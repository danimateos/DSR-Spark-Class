{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f56d38be790>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf metastore_db/*.lck\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-09-26 20:31:47--  https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_libsvm_data.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.12.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.12.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 104736 (102K) [text/plain]\n",
      "Saving to: ‘sample_libsvm_data.txt.2’\n",
      "\n",
      "100%[======================================>] 104.736     --.-K/s   in 0,09s   \n",
      "\n",
      "2016-09-26 20:31:48 (1,10 MB/s) - ‘sample_libsvm_data.txt.2’ saved [104736/104736]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "!wget https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_libsvm_data.txt\n",
    "\n",
    "data = MLUtils.loadLibSVMFile(sc, \"sample_libsvm_data.txt\").toDF()\n",
    "data = MLUtils.convertVectorColumnsToML(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(692,[127,128,129...|  0.0|\n",
      "|(692,[158,159,160...|  1.0|\n",
      "|(692,[124,125,126...|  1.0|\n",
      "|(692,[152,153,154...|  1.0|\n",
      "|(692,[151,152,153...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString, VectorIndexer\n",
    "\n",
    "labelIndexer = StringIndexer().setInputCol(\"label\").setOutputCol(\"indexedLabel\").fit(data)\n",
    "\n",
    "labelConverter = IndexToString().setInputCol(\"prediction\").setOutputCol(\"predictedLabel\").setLabels(labelIndexer.labels)\n",
    "\n",
    "featureIndexer = VectorIndexer().setInputCol(\"features\").setOutputCol(\"indexedFeatures\").setMaxCategories(4).fit(data)\n",
    "\n",
    "dtC = DecisionTreeClassifier().setLabelCol(\"indexedLabel\").setFeaturesCol(\"indexedFeatures\")\n",
    "\n",
    "pipelineClass = Pipeline().setStages([labelIndexer, featureIndexer, dtC, labelConverter])\n",
    "\n",
    "trainingData, testData = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier, DecisionTreeClassificationModel\n",
    "\n",
    "modelClassifier = pipelineClass.fit(trainingData)\n",
    "\n",
    "treeModel = modelClassifier.stages[2]\n",
    "\n",
    "predictionsClass = modelClassifier.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_45e0a3cbec8d55001833,\n",
       " VectorIndexer_49e9bb8409501dddeaec,\n",
       " DecisionTreeClassificationModel (uid=DecisionTreeClassifier_4b9dafd963214991d58f) of depth 2 with 5 nodes,\n",
       " IndexToString_41da84ef0155ffecb971]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelClassifier.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_4b9dafd963214991d58f) of depth 2 with 5 nodes\n",
      "  If (feature 406 <= 20.0)\n",
      "   If (feature 99 in {2.0})\n",
      "    Predict: 0.0\n",
      "   Else (feature 99 not in {2.0})\n",
      "    Predict: 1.0\n",
      "  Else (feature 406 > 20.0)\n",
      "   Predict: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print treeModel.toDebugString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>label</th>\n",
       "      <th>indexedLabel</th>\n",
       "      <th>indexedFeatures</th>\n",
       "      <th>rawPrediction</th>\n",
       "      <th>probability</th>\n",
       "      <th>prediction</th>\n",
       "      <th>predictedLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 27.0]</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 27.0]</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 27.0]</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[37.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[37.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            features  label  indexedLabel  \\\n",
       "0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    0.0           1.0   \n",
       "1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    0.0           1.0   \n",
       "2  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    0.0           1.0   \n",
       "3  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    1.0           0.0   \n",
       "4  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    1.0           0.0   \n",
       "\n",
       "                                     indexedFeatures rawPrediction  \\\n",
       "0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   [0.0, 27.0]   \n",
       "1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   [0.0, 27.0]   \n",
       "2  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   [0.0, 27.0]   \n",
       "3  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   [37.0, 0.0]   \n",
       "4  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   [37.0, 0.0]   \n",
       "\n",
       "  probability  prediction predictedLabel  \n",
       "0  [0.0, 1.0]         1.0            0.0  \n",
       "1  [0.0, 1.0]         1.0            0.0  \n",
       "2  [0.0, 1.0]         1.0            0.0  \n",
       "3  [1.0, 0.0]         0.0            1.0  \n",
       "4  [1.0, 0.0]         0.0            1.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionsClass.toPandas()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor, DecisionTreeRegressionModel\n",
    "\n",
    "dtR = DecisionTreeRegressor().setLabelCol(\"label\").setFeaturesCol(\"indexedFeatures\")\n",
    "\n",
    "pipelineReg = Pipeline().setStages([featureIndexer, dtR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeRegressionModel (uid=DecisionTreeRegressor_4457a5d93742a969bab8) of depth 2 with 5 nodes\n",
      "  If (feature 406 <= 20.0)\n",
      "   If (feature 99 in {0.0})\n",
      "    Predict: 0.0\n",
      "   Else (feature 99 not in {0.0})\n",
      "    Predict: 1.0\n",
      "  Else (feature 406 > 20.0)\n",
      "   Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelRegressor = pipelineReg.fit(trainingData)\n",
    "\n",
    "treeModel = modelRegressor.stages[1]\n",
    "\n",
    "print treeModel.toDebugString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "predictionsReg = modelRegressor.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>label</th>\n",
       "      <th>indexedFeatures</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            features  label  \\\n",
       "0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    0.0   \n",
       "1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    0.0   \n",
       "2  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    0.0   \n",
       "3  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    1.0   \n",
       "4  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    1.0   \n",
       "\n",
       "                                     indexedFeatures  prediction  \n",
       "0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         1.0  \n",
       "1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         0.0  \n",
       "2  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         0.0  \n",
       "3  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         1.0  \n",
       "4  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         1.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionsReg.toPandas()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f7c9758d790>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf metastore_db/*.lck\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "data = MLUtils.loadLibSVMFile(sc, \"sample_libsvm_data.txt\").toDF()\n",
    "data = MLUtils.convertVectorColumnsToML(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(692,[127,128,129...|  0.0|\n",
      "|(692,[158,159,160...|  1.0|\n",
      "|(692,[124,125,126...|  1.0|\n",
      "|(692,[152,153,154...|  1.0|\n",
      "|(692,[151,152,153...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString, VectorIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "\n",
    "labelIndexer = StringIndexer().setInputCol(\"label\") \\\n",
    "                                .setOutputCol(\"indexedLabel\").fit(data)\n",
    "\n",
    "labelConverter = IndexToString().setInputCol(\"prediction\") \\\n",
    "                                .setOutputCol(\"predictedLabel\") \\\n",
    "                                .setLabels(labelIndexer.labels)\n",
    "\n",
    "featureIndexer = VectorIndexer().setInputCol(\"features\") \\\n",
    "                                .setOutputCol(\"indexedFeatures\") \\\n",
    "                                .setMaxCategories(4).fit(data)\n",
    "\n",
    "rfC = RandomForestClassifier().setLabelCol(\"indexedLabel\") \\\n",
    "                                .setFeaturesCol(\"indexedFeatures\") \\\n",
    "                                .setNumTrees(3)\n",
    "        \n",
    "trainingData, testData = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pipelineRFC = Pipeline().setStages([labelIndexer, featureIndexer, rfC, labelConverter])\n",
    "\n",
    "modelRFC = pipelineRFC.fit(trainingData)\n",
    "\n",
    "predictionsRFC = modelRFC.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+--------------------+\n",
      "|predictedLabel|label|            features|\n",
      "+--------------+-----+--------------------+\n",
      "|           1.0|  1.0|(692,[97,98,99,12...|\n",
      "|           0.0|  0.0|(692,[122,123,148...|\n",
      "|           1.0|  1.0|(692,[123,124,125...|\n",
      "|           1.0|  1.0|(692,[123,124,125...|\n",
      "|           0.0|  0.0|(692,[124,125,126...|\n",
      "+--------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionsRFC.select(\"predictedLabel\", \"label\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(692, {183: 0.0326, 455: 0.2952, 463: 0.2636, 490: 0.3333, 517: 0.0371, 540: 0.0381})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfModelC = modelRFC.stages[2]\n",
    "rfModelC.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassificationModel (uid=rfc_d5d6b33eed79) with 3 trees\n",
      "  Tree 0 (weight 1.0):\n",
      "    If (feature 463 <= 0.0)\n",
      "     If (feature 183 <= 0.0)\n",
      "      Predict: 0.0\n",
      "     Else (feature 183 > 0.0)\n",
      "      If (feature 517 <= 116.0)\n",
      "       Predict: 1.0\n",
      "      Else (feature 517 > 116.0)\n",
      "       Predict: 0.0\n",
      "    Else (feature 463 > 0.0)\n",
      "     Predict: 0.0\n",
      "  Tree 1 (weight 1.0):\n",
      "    If (feature 490 <= 31.0)\n",
      "     Predict: 1.0\n",
      "    Else (feature 490 > 31.0)\n",
      "     Predict: 0.0\n",
      "  Tree 2 (weight 1.0):\n",
      "    If (feature 455 <= 23.0)\n",
      "     If (feature 540 <= 65.0)\n",
      "      Predict: 0.0\n",
      "     Else (feature 540 > 65.0)\n",
      "      Predict: 1.0\n",
      "    Else (feature 455 > 23.0)\n",
      "     Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print rfModelC.toDebugString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import RandomForestRegressionModel\n",
    "\n",
    "rfR = RandomForestRegressor().setLabelCol(\"label\").setFeaturesCol(\"indexedFeatures\")\n",
    "\n",
    "pipelineRFR = Pipeline().setStages([featureIndexer, rfR])\n",
    "\n",
    "modelRFR = pipelineRFR.fit(trainingData)\n",
    "\n",
    "predictionsRFR = modelRFR.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.9|  1.0|(692,[97,98,99,12...|\n",
      "|      0.05|  0.0|(692,[122,123,148...|\n",
      "|       1.0|  1.0|(692,[123,124,125...|\n",
      "|      0.95|  1.0|(692,[123,124,125...|\n",
      "|       0.0|  0.0|(692,[124,125,126...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionsRFR = modelRFR.transform(testData)\n",
    "\n",
    "predictionsRFR.select(\"prediction\", \"label\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7ffedec83790>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf metastore_db/*.lck\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "data = MLUtils.loadLibSVMFile(sc, \"sample_libsvm_data.txt\").toDF()\n",
    "data = MLUtils.convertVectorColumnsToML(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(692,[127,128,129...|  0.0|\n",
      "|(692,[158,159,160...|  1.0|\n",
      "|(692,[124,125,126...|  1.0|\n",
      "|(692,[152,153,154...|  1.0|\n",
      "|(692,[151,152,153...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString, VectorIndexer\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import GBTClassificationModel\n",
    "\n",
    "labelIndexer = StringIndexer().setInputCol(\"label\") \\\n",
    "                                .setOutputCol(\"indexedLabel\").fit(data)\n",
    "\n",
    "labelConverter = IndexToString().setInputCol(\"prediction\") \\\n",
    "                                .setOutputCol(\"predictedLabel\") \\\n",
    "                                .setLabels(labelIndexer.labels)\n",
    "\n",
    "featureIndexer = VectorIndexer().setInputCol(\"features\") \\\n",
    "                                .setOutputCol(\"indexedFeatures\") \\\n",
    "                                .setMaxCategories(4).fit(data)\n",
    "\n",
    "gbtC = GBTClassifier().setLabelCol(\"indexedLabel\").setFeaturesCol(\"indexedFeatures\").setMaxIter(10)\n",
    "\n",
    "pipelineGBTC = Pipeline().setStages([labelIndexer, featureIndexer, gbtC, labelConverter])\n",
    "\n",
    "trainingData, testData = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+--------------------+\n",
      "|predictedLabel|label|            features|\n",
      "+--------------+-----+--------------------+\n",
      "|           0.0|  0.0|(692,[122,123,148...|\n",
      "|           1.0|  1.0|(692,[123,124,125...|\n",
      "|           0.0|  0.0|(692,[124,125,126...|\n",
      "+--------------+-----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelGBTC = pipelineGBTC.fit(trainingData)\n",
    "\n",
    "predictionsGBTC = modelGBTC.transform(testData)\n",
    "\n",
    "predictionsGBTC.select(\"predictedLabel\", \"label\", \"features\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBTClassificationModel (uid=GBTClassifier_452db09d245c6ad64f92) with 10 trees\n",
      "  Tree 0 (weight 1.0):\n",
      "    If (feature 406 <= 72.0)\n",
      "     If (feature 99 in {2.0})\n",
      "      Predict: -1.0\n",
      "     Else (feature 99 not in {2.0})\n",
      "      Predict: 1.0\n",
      "    Else (feature 406 > 72.0)\n",
      "     Predict: -1.0\n",
      "  Tree 1 (weight 0.1):\n",
      "    If (feature 406 <= 72.0)\n",
      "     If (feature 435 <= 0.0)\n",
      "      If (feature 577 <= 231.0)\n",
      "       If (feature 123 <= 66.0)\n",
      "        If (feature 153 <= 3.0)\n",
      "         Predict: 0.4768116880884702\n",
      "        Else (feature 153 > 3.0)\n",
      "         Predict: 0.4768116880884703\n",
      "       Else (feature 123 > 66.0)\n",
      "        Predict: 0.4768116880884703\n",
      "      Else (feature 577 > 231.0)\n",
      "       Predict: 0.47681168808847035\n",
      "     Else (feature 435 > 0.0)\n",
      "      Predict: -0.4768116880884694\n",
      "    Else (feature 406 > 72.0)\n",
      "     If (feature 207 <= 140.0)\n",
      "      Predict: -0.47681168808847024\n",
      "     Else (feature 207 > 140.0)\n",
      "      Predict: -0.4768116880884712\n",
      "  Tree 2 (weight 0.1):\n",
      "    If (feature 406 <= 72.0)\n",
      "     If (feature 99 in {2.0})\n",
      "      Predict: -0.4381935810427206\n",
      "     Else (feature 99 not in {2.0})\n",
      "      Predict: 0.4381935810427206\n",
      "    Else (feature 406 > 72.0)\n",
      "     If (feature 517 <= 206.0)\n",
      "      If (feature 322 <= 212.0)\n",
      "       If (feature 240 <= 0.0)\n",
      "        Predict: -0.43819358104272044\n",
      "       Else (feature 240 > 0.0)\n",
      "        Predict: -0.43819358104272066\n",
      "      Else (feature 322 > 212.0)\n",
      "       Predict: -0.43819358104272066\n",
      "     Else (feature 517 > 206.0)\n",
      "      Predict: -0.43819358104272066\n",
      "  Tree 3 (weight 0.1):\n",
      "    If (feature 490 <= 0.0)\n",
      "     If (feature 544 <= 252.0)\n",
      "      If (feature 155 <= 129.0)\n",
      "       Predict: 0.4051496802845983\n",
      "      Else (feature 155 > 129.0)\n",
      "       Predict: 0.40514968028459836\n",
      "     Else (feature 544 > 252.0)\n",
      "      Predict: -0.4051496802845982\n",
      "    Else (feature 490 > 0.0)\n",
      "     Predict: -0.4051496802845982\n",
      "  Tree 4 (weight 0.1):\n",
      "    If (feature 433 <= 0.0)\n",
      "     If (feature 323 <= 251.0)\n",
      "      If (feature 497 <= 3.0)\n",
      "       If (feature 181 <= 0.0)\n",
      "        Predict: 0.3765841318352991\n",
      "       Else (feature 181 > 0.0)\n",
      "        Predict: 0.37658413183529915\n",
      "      Else (feature 497 > 3.0)\n",
      "       Predict: 0.3765841318352994\n",
      "     Else (feature 323 > 251.0)\n",
      "      Predict: -0.3765841318352994\n",
      "    Else (feature 433 > 0.0)\n",
      "     Predict: -0.37658413183529915\n",
      "  Tree 5 (weight 0.1):\n",
      "    If (feature 406 <= 72.0)\n",
      "     If (feature 549 <= 253.0)\n",
      "      Predict: 0.35166478958101\n",
      "     Else (feature 549 > 253.0)\n",
      "      Predict: -0.3516647895810099\n",
      "    Else (feature 406 > 72.0)\n",
      "     If (feature 379 <= 143.0)\n",
      "      If (feature 153 <= 0.0)\n",
      "       Predict: -0.35166478958101005\n",
      "      Else (feature 153 > 0.0)\n",
      "       Predict: -0.3516647895810101\n",
      "     Else (feature 379 > 143.0)\n",
      "      Predict: -0.35166478958101005\n",
      "  Tree 6 (weight 0.1):\n",
      "    If (feature 406 <= 72.0)\n",
      "     If (feature 548 <= 253.0)\n",
      "      If (feature 235 <= 18.0)\n",
      "       Predict: 0.32974984655529926\n",
      "      Else (feature 235 > 18.0)\n",
      "       Predict: 0.3297498465552994\n",
      "     Else (feature 548 > 253.0)\n",
      "      Predict: -0.32974984655530015\n",
      "    Else (feature 406 > 72.0)\n",
      "     If (feature 463 <= 48.0)\n",
      "      If (feature 323 <= 253.0)\n",
      "       Predict: -0.32974984655529926\n",
      "      Else (feature 323 > 253.0)\n",
      "       Predict: -0.32974984655529926\n",
      "     Else (feature 463 > 48.0)\n",
      "      Predict: -0.3297498465552993\n",
      "  Tree 7 (weight 0.1):\n",
      "    If (feature 406 <= 72.0)\n",
      "     If (feature 568 <= 253.0)\n",
      "      If (feature 211 <= 143.0)\n",
      "       If (feature 263 <= 233.0)\n",
      "        Predict: 0.3103372455197956\n",
      "       Else (feature 263 > 233.0)\n",
      "        Predict: 0.3103372455197957\n",
      "      Else (feature 211 > 143.0)\n",
      "       Predict: 0.3103372455197957\n",
      "     Else (feature 568 > 253.0)\n",
      "      Predict: -0.31033724551979525\n",
      "    Else (feature 406 > 72.0)\n",
      "     Predict: -0.31033724551979563\n",
      "  Tree 8 (weight 0.1):\n",
      "    If (feature 406 <= 72.0)\n",
      "     If (feature 568 <= 253.0)\n",
      "      If (feature 568 <= 0.0)\n",
      "       Predict: 0.2930291649125433\n",
      "      Else (feature 568 > 0.0)\n",
      "       Predict: 0.2930291649125434\n",
      "     Else (feature 568 > 253.0)\n",
      "      Predict: -0.29302916491254294\n",
      "    Else (feature 406 > 72.0)\n",
      "     If (feature 267 <= 60.0)\n",
      "      If (feature 119 in {0.0})\n",
      "       Predict: -0.2930291649125433\n",
      "      Else (feature 119 not in {0.0})\n",
      "       Predict: -0.2930291649125434\n",
      "     Else (feature 267 > 60.0)\n",
      "      Predict: -0.29302916491254344\n",
      "  Tree 9 (weight 0.1):\n",
      "    If (feature 490 <= 0.0)\n",
      "     If (feature 293 <= 253.0)\n",
      "      If (feature 182 <= 99.0)\n",
      "       If (feature 100 <= 0.0)\n",
      "        Predict: 0.27750666438358246\n",
      "       Else (feature 100 > 0.0)\n",
      "        Predict: 0.2775066643835825\n",
      "      Else (feature 182 > 99.0)\n",
      "       Predict: 0.27750666438358257\n",
      "     Else (feature 293 > 253.0)\n",
      "      Predict: -0.27750666438358174\n",
      "    Else (feature 490 > 0.0)\n",
      "     If (feature 490 <= 189.0)\n",
      "      Predict: -0.2775066643835825\n",
      "     Else (feature 490 > 189.0)\n",
      "      Predict: -0.27750666438358257\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gbtModelC = modelGBTC.stages[2]\n",
    "\n",
    "print gbtModelC.toDebugString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.regression import GBTRegressionModel\n",
    "\n",
    "gbtR = GBTRegressor().setLabelCol(\"label\").setFeaturesCol(\"indexedFeatures\").setMaxIter(10)\n",
    "\n",
    "pipelineGBTR = Pipeline().setStages([featureIndexer, gbtR])\n",
    "\n",
    "modelGBTR = pipelineGBTR.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+----------+\n",
      "|            features|label|     indexedFeatures|prediction|\n",
      "+--------------------+-----+--------------------+----------+\n",
      "|(692,[122,123,148...|  0.0|(692,[122,123,148...|       0.0|\n",
      "|(692,[123,124,125...|  1.0|(692,[123,124,125...|       1.0|\n",
      "|(692,[124,125,126...|  0.0|(692,[124,125,126...|       0.0|\n",
      "|(692,[124,125,126...|  0.0|(692,[124,125,126...|       0.0|\n",
      "|(692,[124,125,126...|  1.0|(692,[124,125,126...|       1.0|\n",
      "+--------------------+-----+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionsGBTR = modelGBTR.transform(testData)\n",
    "predictionsGBTR.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7fe825ff8790>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "data = MLUtils.loadLibSVMFile(sc, \"sample_libsvm_data.txt\").toDF()\n",
    "data = MLUtils.convertVectorColumnsToML(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(692,[127,128,129...|  0.0|\n",
      "|(692,[158,159,160...|  1.0|\n",
      "|(692,[124,125,126...|  1.0|\n",
      "|(692,[152,153,154...|  1.0|\n",
      "|(692,[151,152,153...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Linear Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: (692,[235,243,244,262,263,271,272,290,300,328,345,350,351,373,378,379,400,401,405,406,407,428,429,433,434,455,456,461,462,468,483,484,489,490,496,511,512,517,539,540,568],[-0.000366498963754,-1.42795225616e-05,-9.44060912441e-07,-0.000205590003336,-0.000640271226841,-0.00015049613407,-4.82707085543e-05,-0.000149047540375,-0.00024110937152,-3.31762123523e-05,-8.66627952077e-06,9.14036907531e-05,7.91821174721e-05,-0.000217747702831,0.000506741491092,8.12357667511e-05,-3.70834836804e-05,-0.000248022863756,0.000180277568249,0.000670911954862,0.000199036991568,-0.000248992981369,-4.4187919864e-05,0.000534981568858,0.000602296780737,-2.46888169952e-05,-0.000226660517233,0.000178721956572,0.000526650147849,-2.65336833343e-05,-0.000229259905073,-4.95566431909e-05,0.000170380977959,6.28404067389e-05,-0.000148349494778,-0.000431896225065,-0.000217977284272,6.88366318543e-05,-0.000393900871055,-0.000372365159358,-0.000180633333959]) Intercept: 0.311873962216\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "trainingData, testData = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "logr = LogisticRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "\n",
    "logrModel = logr.fit(trainingData)\n",
    "\n",
    "print \"Weights: %s Intercept: %s\" % (logrModel.coefficients, logrModel.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "trainingSummaryLR = logrModel.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|FPR|                 TPR|\n",
      "+---+--------------------+\n",
      "|0.0|                 0.0|\n",
      "|0.0|0.023809523809523808|\n",
      "|0.0|0.047619047619047616|\n",
      "|0.0| 0.07142857142857142|\n",
      "|0.0| 0.09523809523809523|\n",
      "|0.0| 0.11904761904761904|\n",
      "|0.0| 0.14285714285714285|\n",
      "|0.0| 0.16666666666666666|\n",
      "|0.0| 0.19047619047619047|\n",
      "|0.0| 0.21428571428571427|\n",
      "|0.0| 0.23809523809523808|\n",
      "|0.0|  0.2619047619047619|\n",
      "|0.0|  0.2857142857142857|\n",
      "|0.0| 0.30952380952380953|\n",
      "|0.0|  0.3333333333333333|\n",
      "|0.0| 0.35714285714285715|\n",
      "|0.0| 0.38095238095238093|\n",
      "|0.0| 0.40476190476190477|\n",
      "|0.0| 0.42857142857142855|\n",
      "|0.0|  0.4523809523809524|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingSummaryLR.roc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingSummaryLR.areaUnderROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: (692,[235,262,263,271,272,290,300,323,328,345,350,351,373,377,378,379,400,401,405,406,407,428,429,433,434,435,455,456,461,462,468,483,484,489,490,496,511,512,517,539,540,568],[-2.09476863284e-05,-1.80379795203e-05,-7.76408069301e-05,-8.62510909972e-06,-8.8863598712e-06,-1.63638781455e-05,-2.32113528661e-05,2.07824914011e-05,-6.63419413635e-06,-1.31267310255e-06,9.73660810406e-05,8.67848065341e-05,-2.0818468715e-05,1.69492814487e-05,0.000127924916667,8.73625991595e-05,-8.12877145313e-06,-2.44097080685e-05,0.000105190944195,0.000193955183142,0.000117112902109,-2.38297092758e-05,-8.36554299431e-06,0.000131433521016,0.000184741250333,2.22045575455e-05,-5.96199550215e-06,-2.17969886225e-05,0.000103310686136,0.000129828235781,-5.20120442642e-06,-2.13971309011e-05,-9.62497812109e-06,9.76958909363e-05,8.09841251226e-05,-9.43615158764e-06,-7.2861938126e-05,-2.08047656197e-05,8.45794820099e-05,-6.40729390395e-05,-2.51503732101e-05,-1.55360772201e-05]) Intercept: 0.40386219884\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "\n",
    "lrModel = lr.fit(trainingData)\n",
    "\n",
    "print \"Weights: %s Intercept: %s\" % (lrModel.coefficients, lrModel.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numIterations: 11\n"
     ]
    }
   ],
   "source": [
    "trainingSummaryLLS = lrModel.summary\n",
    "\n",
    "print \"numIterations: %s\" % trainingSummaryLLS.totalIterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           residuals|\n",
      "+--------------------+\n",
      "|-0.28172778090489775|\n",
      "|  0.5072260030209965|\n",
      "| 0.31386479014178303|\n",
      "| 0.20612375835211627|\n",
      "|  0.2894883399407576|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingSummaryLLS.residuals.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7fa288754790>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf metastore_db/*.lck\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "data = MLUtils.loadLibSVMFile(sc, \"sample_libsvm_data.txt\").toDF()\n",
    "data = MLUtils.convertVectorColumnsToML(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(692,[127,128,129...|  0.0|\n",
      "|(692,[158,159,160...|  1.0|\n",
      "|(692,[124,125,126...|  1.0|\n",
      "|(692,[152,153,154...|  1.0|\n",
      "|(692,[151,152,153...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: (692,[235,243,244,262,263,271,272,290,300,301,323,328,329,350,351,358,378,379,385,386,401,405,406,407,413,427,428,429,433,434,435,455,456,461,462,469,483,484,489,490,496,497,511,512,517,539,540,568],[-1.08704076463e-05,-0.000225035191295,-1.42785721134e-06,-2.67838954428e-06,-0.000260749167911,-0.000108151886583,-0.000222047179992,-0.000184684690252,-0.000368205319861,-8.45317828463e-05,8.41700764597e-05,-8.99150674055e-06,-2.6018144065e-06,0.000106542521448,0.000214847611674,-8.02001292779e-06,0.000254341232325,0.000218251840905,-3.96099160439e-05,-2.89412079021e-05,-4.45357563079e-06,5.02740058996e-05,0.00084364056682,0.000260914756075,-2.13472989153e-05,-1.2101806874e-06,-0.000135806709886,-9.35571716832e-06,0.000231569631081,0.000943064886038,7.40416456292e-05,-1.3984003133e-05,-0.000254944102754,0.000131412325807,0.000884111338263,-9.88044455622e-05,-8.10057764206e-05,-0.000193685045759,0.00011911644026,0.000142456462059,-0.000123316509266,-8.01517008313e-05,-0.000390099390576,-0.000394491923359,0.00016643986746,-0.000267852255111,-0.000848200730725,-0.000151726889697]) Intercept: 0.297623966973\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import BinaryLogisticRegressionSummary\n",
    "\n",
    "trainingData, testData = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "logr = LogisticRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "\n",
    "logrModel = logr.fit(trainingData)\n",
    "\n",
    "print \"Weights: %s Intercept: %s\" % (logrModel.coefficients, logrModel.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "predictionsLogR = logrModel.transform(testData)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator().setLabelCol(\"label\") \\\n",
    "                            .setRawPredictionCol(\"rawPrediction\") \\\n",
    "                            .setMetricName(\"areaUnderROC\")\n",
    "\n",
    "roc = evaluator.evaluate(predictionsLogR)\n",
    "print roc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString, VectorIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "\n",
    "labelIndexer = StringIndexer().setInputCol(\"label\") \\\n",
    "                                .setOutputCol(\"indexedLabel\").fit(data)\n",
    "\n",
    "labelConverter = IndexToString().setInputCol(\"prediction\") \\\n",
    "                                .setOutputCol(\"predictedLabel\") \\\n",
    "                                .setLabels(labelIndexer.labels)\n",
    "\n",
    "featureIndexer = VectorIndexer().setInputCol(\"features\") \\\n",
    "                                .setOutputCol(\"indexedFeatures\") \\\n",
    "                                .setMaxCategories(4).fit(data)\n",
    "\n",
    "rfC = RandomForestClassifier().setLabelCol(\"indexedLabel\") \\\n",
    "                                .setFeaturesCol(\"indexedFeatures\") \\\n",
    "                                .setNumTrees(3)\n",
    "        \n",
    "pipelineRFC = Pipeline().setStages([labelIndexer, featureIndexer, rfC, labelConverter])\n",
    "\n",
    "modelRFC = pipelineRFC.fit(trainingData)\n",
    "\n",
    "predictionsRFC = modelRFC.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.0606060606061\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator().setLabelCol(\"indexedLabel\") \\\n",
    "                                        .setPredictionCol(\"prediction\") \\\n",
    "                                        .setMetricName(\"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictionsRFC)\n",
    "\n",
    "print \"Test Error = %s\" % (1.0 - accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import RandomForestRegressionModel\n",
    "\n",
    "rfR = RandomForestRegressor().setLabelCol(\"label\").setFeaturesCol(\"indexedFeatures\")\n",
    "\n",
    "pipelineRFR = Pipeline().setStages([featureIndexer, rfR])\n",
    "\n",
    "modelRFR = pipelineRFR.fit(trainingData)\n",
    "\n",
    "predictionsRFR = modelRFR.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) = 0.138169855942\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator().setLabelCol(\"label\") \\\n",
    "                                .setPredictionCol(\"prediction\") \\\n",
    "                                .setMetricName(\"rmse\")\n",
    "\n",
    "rmse = evaluator.evaluate(predictionsRFR)\n",
    "\n",
    "print \"Root Mean Squared Error (RMSE) = %s\" % rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: (692,[235,243,244,262,263,271,272,290,300,301,323,328,329,350,351,358,378,379,385,386,401,405,406,407,413,427,428,429,433,434,435,455,456,461,462,469,483,484,489,490,496,497,511,512,517,539,540,568],[-1.08704076463e-05,-0.000225035191295,-1.42785721134e-06,-2.67838954428e-06,-0.000260749167911,-0.000108151886583,-0.000222047179992,-0.000184684690252,-0.000368205319861,-8.45317828463e-05,8.41700764597e-05,-8.99150674055e-06,-2.6018144065e-06,0.000106542521448,0.000214847611674,-8.02001292779e-06,0.000254341232325,0.000218251840905,-3.96099160439e-05,-2.89412079021e-05,-4.45357563079e-06,5.02740058996e-05,0.00084364056682,0.000260914756075,-2.13472989153e-05,-1.2101806874e-06,-0.000135806709886,-9.35571716832e-06,0.000231569631081,0.000943064886038,7.40416456292e-05,-1.3984003133e-05,-0.000254944102754,0.000131412325807,0.000884111338263,-9.88044455622e-05,-8.10057764206e-05,-0.000193685045759,0.00011911644026,0.000142456462059,-0.000123316509266,-8.01517008313e-05,-0.000390099390576,-0.000394491923359,0.00016643986746,-0.000267852255111,-0.000848200730725,-0.000151726889697]) Intercept: 0.297623966973\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "logr = LogisticRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "\n",
    "logrModel = logr.fit(trainingData)\n",
    "\n",
    "print \"Weights: %s Intercept: %s\" % (logrModel.coefficients, logrModel.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingSummaryLR = logrModel.summary\n",
    "trainingSummaryLR.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+\n",
      "|         threshold|          F-Measure|\n",
      "+------------------+-------------------+\n",
      "|0.8165428632958064|               0.05|\n",
      "|0.8162694676199894|0.09756097560975609|\n",
      "|0.8162646945004736|0.14285714285714288|\n",
      "+------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fMeasure = trainingSummaryLR.fMeasureByThreshold\n",
    "\n",
    "fMeasure.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "0.613903946897\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "maxFMeasure = fMeasure.agg({\"F-Measure\": \"max\"}).head()[0]\n",
    "print maxFMeasure\n",
    "maxFMeasure = fMeasure.agg(F.max(F.col(\"F-Measure\"))).head()[0]\n",
    "print maxFMeasure\n",
    "\n",
    "bestThreshold = fMeasure.where(F.col(\"F-Measure\") == maxFMeasure).select(\"threshold\").head()[0]\n",
    "print bestThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+\n",
      "|             recall|precision|\n",
      "+-------------------+---------+\n",
      "|                0.0|      1.0|\n",
      "|0.02564102564102564|      1.0|\n",
      "|0.05128205128205128|      1.0|\n",
      "+-------------------+---------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------------------+---------+\n",
      "|         threshold|precision|\n",
      "+------------------+---------+\n",
      "|0.8165428632958064|      1.0|\n",
      "|0.8162694676199894|      1.0|\n",
      "|0.8162646945004736|      1.0|\n",
      "+------------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingSummaryLR.pr.show(3)\n",
    "trainingSummaryLR.precisionByThreshold.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+\n",
      "|         threshold|             recall|\n",
      "+------------------+-------------------+\n",
      "|0.8165428632958064|0.02564102564102564|\n",
      "|0.8162694676199894|0.05128205128205128|\n",
      "|0.8162646945004736|0.07692307692307693|\n",
      "+------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---+-------------------+\n",
      "|FPR|                TPR|\n",
      "+---+-------------------+\n",
      "|0.0|                0.0|\n",
      "|0.0|0.02564102564102564|\n",
      "|0.0|0.05128205128205128|\n",
      "+---+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingSummaryLR.recallByThreshold.show(3)\n",
    "trainingSummaryLR.roc.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: (692,[351,378,379,406,407,433,434,462,490,517,540],[0.000133767304687,0.00018419522405,0.000135554811074,0.000275780406915,0.000159724075215,0.000144485199988,0.000306915977164,0.000288535540884,0.000120301061362,0.000123576266478,-0.00011823301358]) Intercept: 0.34681682126\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "\n",
    "lrModel = lr.fit(trainingData)\n",
    "\n",
    "print \"Weights: %s Intercept: %s\" % (lrModel.coefficients, lrModel.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0506615309323\n",
      "0.271181060072\n",
      "0.0785812855913\n",
      "0.676967590642\n"
     ]
    }
   ],
   "source": [
    "trainingSummaryLLS = lrModel.summary\n",
    "\n",
    "print trainingSummaryLLS.explainedVariance\n",
    "\n",
    "print trainingSummaryLLS.meanAbsoluteError\n",
    "\n",
    "print trainingSummaryLLS.meanSquaredError\n",
    "\n",
    "print trainingSummaryLLS.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "|0.24739337680570395|\n",
      "|-0.3169038688245795|\n",
      "| 0.5361913707012146|\n",
      "+-------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "0.280323537348\n"
     ]
    }
   ],
   "source": [
    "trainingSummaryLLS.residuals.show(3)\n",
    "\n",
    "print trainingSummaryLLS.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7fbeaf4da790>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf metastore_db/*.lck\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Saving and Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "df = sqlc.createDataFrame([(1.0, Vectors.dense(1.0, 2.0, 3.0)),\n",
    "                           (1.0, Vectors.dense(2.0, 3.0, 4.0)),\n",
    "                           (0.0, Vectors.dense(-1.0, 1.0, 2.0)),\n",
    "                           (0.0, Vectors.dense(-2.0, 3.0, 5.0))]).toDF(\"label\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "|label|      features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "|  1.0| [1.0,2.0,3.0]|[-18.070405604445...|[1.41945802370848...|       1.0|\n",
      "|  1.0| [2.0,3.0,4.0]|[-38.987081234651...|[1.16983808020729...|       1.0|\n",
      "|  0.0|[-1.0,1.0,2.0]|[19.2085506510254...|[0.99999999545187...|       0.0|\n",
      "|  0.0|[-2.0,3.0,5.0]|[29.1902958840818...|[0.99999999999978...|       0.0|\n",
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lrModel = lr.fit(df)\n",
    "lrModel.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lrModel.save(\"lrModel.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "|label|      features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "|  1.0| [1.0,2.0,3.0]|[-18.070405604445...|[1.41945802370848...|       1.0|\n",
      "|  1.0| [2.0,3.0,4.0]|[-38.987081234651...|[1.16983808020729...|       1.0|\n",
      "|  0.0|[-1.0,1.0,2.0]|[19.2085506510254...|[0.99999999545187...|       0.0|\n",
      "|  0.0|[-2.0,3.0,5.0]|[29.1902958840818...|[0.99999999999978...|       0.0|\n",
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "\n",
    "sameModel = LogisticRegressionModel.load(\"lrModel.parquet\")\n",
    "sameModel.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"class\":\"org.apache.spark.ml.classification.LogisticRegressionModel\",\"timestamp\":1475089043445,\"sparkVersion\":\"2.0.0\",\"uid\":\"LogisticRegression_43d69252fb4e6810e320\",\"paramMap\":{\"regParam\":0.0,\"tol\":1.0E-6,\"fitIntercept\":true,\"maxIter\":100,\"standardization\":true,\"elasticNetParam\":0.0,\"probabilityCol\":\"probability\",\"rawPredictionCol\":\"rawPrediction\",\"featuresCol\":\"features\",\"labelCol\":\"label\",\"predictionCol\":\"prediction\",\"threshold\":0.5}}\r\n"
     ]
    }
   ],
   "source": [
    "!cat lrModel.parquet/metadata/part-00000 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
