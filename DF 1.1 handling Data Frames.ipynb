{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7fa148b867d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!rm ./metastore_db/*.lck\n",
    "examples_folder = \"/home/ubuntu/spark/examples/src/main/resources/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlc.read.load(examples_folder + \"users.parquet\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf namesAndFavColors.parquet/\n",
    "df.select(\"name\", \"favorite_color\").write.save(\"namesAndFavColors.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4\r\n",
      "-rw-r--r-- 1 ubuntu ubuntu 572 Jan 24 12:28 part-00000-48237f36-a161-40f0-9a6e-be1e0de7825a.snappy.parquet\r\n",
      "-rw-r--r-- 1 ubuntu ubuntu   0 Jan 24 12:28 _SUCCESS\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l namesAndFavColors.parquet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, favorite_color: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlc.read.format(\"org.apache.spark.sql.parquet\").load(\"namesAndFavColors.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, favorite_color: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlc.read.format(\"parquet\").load(\"namesAndFavColors.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, favorite_color: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlc.read.parquet(\"namesAndFavColors.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## JSON Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_json = sqlc.read.format(\"json\").load(examples_folder + \"people.json\")\n",
    "df_json.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_json.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'{\"name\":\"Michael\"}',\n",
       " u'{\"age\":30,\"name\":\"Andy\"}',\n",
       " u'{\"age\":19,\"name\":\"Justin\"}']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_strings = df_json.toJSON()\n",
    "json_strings.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf namesAndAges.parquet/\n",
    "df_json.select(\"name\", \"age\").write.format(\"parquet\").save(\"namesAndAges.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4\r\n",
      "-rw-r--r-- 1 ubuntu ubuntu 585 Jan 24 12:28 part-00000-e91075df-0f69-4e59-a132-fc28ad3422be.snappy.parquet\r\n",
      "-rw-r--r-- 1 ubuntu ubuntu   0 Jan 24 12:28 _SUCCESS\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l namesAndAges.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf namesAndAgesCoalesced.parquet/\n",
    "df_json.select(\"name\", \"age\").coalesce(1).write.format(\"parquet\").save(\"namesAndAgesCoalesced.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4\r\n",
      "-rw-r--r-- 1 ubuntu ubuntu 585 Jan 24 12:28 part-00000-9815aeb8-35d7-4eaa-bfd5-4f3f70da0570.snappy.parquet\r\n",
      "-rw-r--r-- 1 ubuntu ubuntu   0 Jan 24 12:28 _SUCCESS\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l namesAndAgesCoalesced.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "u'path file:/home/ubuntu/AUDI-SPARK-TRAINING/namesAndAges.parquet already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-ec97fac0c2b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_json\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"namesAndAges.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# This IS supposed to throw an exception, as it is trying to write an already existent file!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/spark/python/pyspark/sql/readwriter.pyc\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/spark/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u'path file:/home/ubuntu/AUDI-SPARK-TRAINING/namesAndAges.parquet already exists.;'"
     ]
    }
   ],
   "source": [
    "df_json.write.mode(\"error\").parquet(\"namesAndAges.parquet\")\n",
    "# This IS supposed to throw an exception, as it is trying to write an already existent file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_json.write.mode(\"overwrite\").parquet(\"namesAndAges.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SQL on Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age|  name|\n",
      "+---+------+\n",
      "| 19|Justin|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlc.sql(\"SELECT * FROM json.`/home/ubuntu/spark/examples/src/main/resources/people.json` WHERE age = 19\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MySQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### We need to run these commands in MYSQL to create the table:\n",
    "\n",
    "```SQL\n",
    "CREATE TABLE users (user_id int PRIMARY KEY, fname text, lname text);\n",
    "INSERT INTO users (user_id,  fname, lname) VALUES (1, 'john', 'smith');\n",
    "INSERT INTO users (user_id,  fname, lname) VALUES (2, 'john', 'doe');\n",
    "INSERT INTO users (user_id,  fname, lname) VALUES (3, 'john', 'smith');\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting mysql (via systemctl): mysql.service.\n"
     ]
    }
   ],
   "source": [
    "# Start the SQL server:\n",
    "\n",
    "!/etc/init.d/mysql start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mysql: [Warning] Using a password on the command line interface can be insecure.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Run the above commands as root\n",
    "mysql -u root --password=root << EOF\n",
    "CREATE DATABASE IF NOT EXISTS db;\n",
    "USE db;\n",
    "DROP TABLE IF EXISTS users;\n",
    "CREATE TABLE users (user_id int PRIMARY KEY, fname text, lname text);\n",
    "INSERT INTO users (user_id,  fname, lname) VALUES (1, 'john', 'smith');\n",
    "INSERT INTO users (user_id,  fname, lname) VALUES (2, 'john', 'doe');\n",
    "INSERT INTO users (user_id,  fname, lname) VALUES (3, 'john', 'smith');\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df_mysql = sqlc.read.format(\"jdbc\") \\\n",
    "                .option(\"url\", \"jdbc:mysql://localhost:3306/db\") \\\n",
    "                .option(\"driver\", \"com.mysql.jdbc.Driver\") \\\n",
    "                .option(\"dbtable\", \"users\") \\\n",
    "                .option(\"user\", \"root\") \\\n",
    "                .option(\"password\", \"root\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+\n",
      "|user_id|fname|lname|\n",
      "+-------+-----+-----+\n",
      "|      1| john|smith|\n",
      "|      2| john|  doe|\n",
      "|      3| john|smith|\n",
      "+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mysql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = false)\n",
      " |-- fname: string (nullable = true)\n",
      " |-- lname: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mysql.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_mysql.registerTempTable(\"users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_id=1, fname=u'john', lname=u'smith'),\n",
       " Row(user_id=2, fname=u'john', lname=u'doe'),\n",
       " Row(user_id=3, fname=u'john', lname=u'smith')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlc.sql(\"select * from users\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7fa148b867d0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!rm ./metastore_db/*.lck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-01-24 12:28:50--  https://github.com/databricks/spark-csv/raw/master/src/test/resources/cars.csv\n",
      "Resolving github.com (github.com)... 192.30.253.112, 192.30.253.113\n",
      "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/databricks/spark-csv/master/src/test/resources/cars.csv [following]\n",
      "--2017-01-24 12:28:51--  https://raw.githubusercontent.com/databricks/spark-csv/master/src/test/resources/cars.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.112.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.112.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 134 [text/plain]\n",
      "Saving to: ‘cars.csv.2’\n",
      "\n",
      "cars.csv.2          100%[===================>]     134  --.-KB/s    in 0s      \n",
      "\n",
      "2017-01-24 12:28:51 (17,1 MB/s) - ‘cars.csv.2’ saved [134/134]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/databricks/spark-csv/raw/master/src/test/resources/cars.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_cars = sqlc.read.format(\"com.databricks.spark.csv\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .option(\"inferSchema\", \"true\") \\\n",
    "                .load(\"cars.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+--------------------+-----+\n",
      "|year| make|model|             comment|blank|\n",
      "+----+-----+-----+--------------------+-----+\n",
      "|2012|Tesla|    S|          No comment| null|\n",
      "|1997| Ford| E350|Go get one now th...| null|\n",
      "|2015|Chevy| Volt|                null| null|\n",
      "+----+-----+-----+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cars.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- make: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- comment: string (nullable = true)\n",
      " |-- blank: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cars.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "customSchema = StructType([StructField(\"year\", StringType(), True),\n",
    "                           StructField(\"make\", StringType(), True),\n",
    "                           StructField(\"model\", StringType(), True), \n",
    "                           StructField(\"comment\", StringType(), True),\n",
    "                           StructField(\"blank\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_cars2 = sqlc.read.load(path=\"cars.csv\", \n",
    "                          format=\"com.databricks.spark.csv\", \n",
    "                          schema=customSchema,\n",
    "                          header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- make: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- comment: string (nullable = true)\n",
      " |-- blank: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cars2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf newcars.csv\n",
    "\n",
    "selectedData = df_cars.select(\"year\", \"model\",\"comment\")\n",
    "selectedData.coalesce(1).write.format(\"com.databricks.spark.csv\") \\\n",
    "                        .option(\"header\", \"true\") \\\n",
    "                        .option(\"nullValue\",\"NA\") \\\n",
    "                        .save(\"newcars.csv\") \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4\r\n",
      "-rw-r--r-- 1 ubuntu ubuntu 95 Jan 24 12:29 part-00000-e4a4e5b0-448d-41cb-bc7b-cca88c6ed50c.csv\r\n",
      "-rw-r--r-- 1 ubuntu ubuntu  0 Jan 24 12:29 _SUCCESS\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l newcars.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf newcars.csv.gz\n",
    "selectedData.write.format(\"com.databricks.spark.csv\") \\\n",
    "                    .option(\"header\", \"true\") \\\n",
    "                    .option(\"codec\", \"gzip\") \\\n",
    "                    .save(\"newcars.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4\r\n",
      "-rw-r--r-- 1 ubuntu ubuntu 104 Jan 24 12:29 part-00000-c4daed4d-6af4-4eeb-b42a-64edb357ae29.csv.gz\r\n",
      "-rw-r--r-- 1 ubuntu ubuntu   0 Jan 24 12:29 _SUCCESS\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l newcars.csv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-01-24 12:29:07--  https://github.com/databricks/spark-xml/raw/master/src/test/resources/books.xml\n",
      "Resolving github.com (github.com)... 192.30.253.112, 192.30.253.113\n",
      "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/databricks/spark-xml/master/src/test/resources/books.xml [following]\n",
      "--2017-01-24 12:29:08--  https://raw.githubusercontent.com/databricks/spark-xml/master/src/test/resources/books.xml\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.112.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.112.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5542 (5,4K) [text/plain]\n",
      "Saving to: ‘books.xml.2’\n",
      "\n",
      "books.xml.2         100%[===================>]   5,41K  --.-KB/s    in 0s      \n",
      "\n",
      "2017-01-24 12:29:08 (109 MB/s) - ‘books.xml.2’ saved [5542/5542]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/databricks/spark-xml/raw/master/src/test/resources/books.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\"?>\r\n",
      "<catalog>\r\n",
      "   <book id=\"bk101\">\r\n",
      "      <author>Gambardella, Matthew</author>\r\n",
      "      <title>XML Developer's Guide</title>\r\n",
      "      <genre>Computer</genre>\r\n",
      "      <price>44.95</price>\r\n",
      "      <publish_date>2000-10-01</publish_date>\r\n",
      "      <description>\r\n",
      "\r\n",
      "\r\n",
      "         An in-depth look at creating applications\r\n",
      "         with XML.This manual describes Oracle XML DB, and how you can use it to store, generate, manipulate, manage,\r\n",
      "         and query XML data in the database.\r\n",
      "\r\n",
      "\r\n",
      "         After introducing you to the heart of Oracle XML DB, namely the XMLType framework and Oracle XML DB repository,\r\n",
      "         the manual provides a brief introduction to design criteria to consider when planning your Oracle XML DB\r\n",
      "         application. It provides examples of how and where you can use Oracle XML DB.\r\n",
      "\r\n",
      "\r\n",
      "         The manual then describes ways you can store and retrieve XML data using Oracle XML DB, APIs for manipulating\r\n",
      "         XMLType data, and ways you can view, generate, transform, and search on existing XML data. The remainder of\r\n",
      "         the manual discusses how to use Oracle XML DB repository, including versioning and security,\r\n",
      "         how to access and manipulate repository resources using protocols, SQL, PL/SQL, or Java, and how to manage\r\n",
      "         your Oracle XML DB application using Oracle Enterprise Manager. It also introduces you to XML messaging and\r\n",
      "         Oracle Streams Advanced Queuing XMLType support.\r\n",
      "      </description></book><book id=\"bk102\">\r\n",
      "      <author>Ralls, Kim</author>\r\n",
      "      <title>Midnight Rain</title>\r\n",
      "      <genre>Fantasy</genre>\r\n",
      "      <price>5.95</price>\r\n",
      "      <publish_date>2000-12-16</publish_date>\r\n",
      "      <description>A former architect battles corporate zombies, \r\n",
      "      an evil sorceress, and her own childhood to become queen \r\n",
      "      of the world.</description>\r\n",
      "   </book>\r\n",
      "   <book id=\"bk103\">\r\n",
      "      <author>Corets, Eva</author>\r\n",
      "      <title>Maeve Ascendant</title>\r\n",
      "      <genre>Fantasy</genre>\r\n",
      "      <price>5.95</price>\r\n",
      "      <publish_date>2000-11-17</publish_date>\r\n",
      "      <description>After the collapse of a nanotechnology \r\n",
      "      society in England, the young survivors lay the \r\n",
      "      foundation for a new society.</description>\r\n",
      "   </book>\r\n",
      "   <book id=\"bk104\">\r\n",
      "      <author>Corets, Eva</author>\r\n",
      "      <title>Oberon's Legacy</title>\r\n",
      "      <genre>Fantasy</genre>\r\n",
      "      <price>5.95</price>\r\n",
      "      <publish_date>2001-03-10</publish_date>\r\n",
      "      <description>In post-apocalypse England, the mysterious \r\n",
      "      agent known only as Oberon helps to create a new life \r\n",
      "      for the inhabitants of London. Sequel to Maeve \r\n",
      "      Ascendant.</description>\r\n",
      "   </book>\r\n",
      "   <book id=\"bk105\">\r\n",
      "      <author>Corets, Eva</author>\r\n",
      "      <title>The Sundered Grail</title>\r\n",
      "      <genre>Fantasy</genre>\r\n",
      "      <price>5.95</price>\r\n",
      "      <publish_date>2001-09-10</publish_date>\r\n",
      "      <description>The two daughters of Maeve, half-sisters, \r\n",
      "      battle one another for control of England. Sequel to \r\n",
      "      Oberon's Legacy.</description>\r\n",
      "   </book>\r\n",
      "   <book id=\"bk106\">\r\n",
      "      <author>Randall, Cynthia</author>\r\n",
      "      <title>Lover Birds</title>\r\n",
      "      <genre>Romance</genre>\r\n",
      "      <price>4.95</price>\r\n",
      "      <publish_date>2000-09-02</publish_date>\r\n",
      "      <description>When Carla meets Paul at an ornithology \r\n",
      "      conference, tempers fly as feathers get ruffled.</description>\r\n",
      "   </book>\r\n",
      "   <book id=\"bk107\">\r\n",
      "      <author>Thurman, Paula</author>\r\n",
      "      <title>Splish Splash</title>\r\n",
      "      <genre>Romance</genre>\r\n",
      "      <price>4.95</price>\r\n",
      "      <publish_date>2000-11-02</publish_date>\r\n",
      "      <description>A deep sea diver finds true love twenty \r\n",
      "      thousand leagues beneath the sea.</description>\r\n",
      "   </book>\r\n",
      "   <book id=\"bk108\">\r\n",
      "      <author>Knorr, Stefan</author>\r\n",
      "      <title>Creepy Crawlies</title>\r\n",
      "      <genre>Horror</genre>\r\n",
      "      <price>4.95</price>\r\n",
      "      <publish_date>2000-12-06</publish_date>\r\n",
      "      <description>An anthology of horror stories about roaches,\r\n",
      "      centipedes, scorpions  and other insects.</description>\r\n",
      "   </book>\r\n",
      "   <book id=\"bk109\">\r\n",
      "      <author>Kress, Peter</author>\r\n",
      "      <title>Paradox Lost</title>\r\n",
      "      <genre>Science Fiction</genre>\r\n",
      "      <price>6.95</price>\r\n",
      "      <publish_date>2000-11-02</publish_date>\r\n",
      "      <description>After an inadvertant trip through a Heisenberg\r\n",
      "      Uncertainty Device, James Salway discovers the problems \r\n",
      "      of being quantum.</description>\r\n",
      "   </book>\r\n",
      "   <book id=\"bk110\">\r\n",
      "      <author>O'Brien, Tim</author>\r\n",
      "      <title>Microsoft .NET: The Programming Bible</title>\r\n",
      "      <genre>Computer</genre>\r\n",
      "      <price>36.95</price>\r\n",
      "      <publish_date>2000-12-09</publish_date>\r\n",
      "      <description>Microsoft's .NET initiative is explored in \r\n",
      "      detail in this deep programmer's reference.</description>\r\n",
      "   </book>\r\n",
      "   <book id=\"bk111\">\r\n",
      "      <author>O'Brien, Tim</author>\r\n",
      "      <title>MSXML3: A Comprehensive Guide</title>\r\n",
      "      <genre>Computer</genre>\r\n",
      "      <price>36.95</price>\r\n",
      "      <publish_date>2000-12-01</publish_date>\r\n",
      "      <description>The Microsoft MSXML3 parser is covered in \r\n",
      "      detail, with attention to XML DOM interfaces, XSLT processing, \r\n",
      "      SAX and more.</description>\r\n",
      "   </book>\r\n",
      "   <book id=\"bk112\">\r\n",
      "      <author>Galos, Mike</author>\r\n",
      "      <title>Visual Studio 7: A Comprehensive Guide</title>\r\n",
      "      <genre>Computer</genre>\r\n",
      "      <price>49.95</price>\r\n",
      "      <publish_date>2001-04-16</publish_date>\r\n",
      "      <description>Microsoft Visual Studio 7 is explored in depth,\r\n",
      "      looking at how Visual Basic, Visual C++, C#, and ASP+ are \r\n",
      "      integrated into a comprehensive development \r\n",
      "      environment.</description>\r\n",
      "   </book>\r\n",
      "</catalog>\r\n"
     ]
    }
   ],
   "source": [
    "!cat books.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df_books = sqlc.read.format(\"com.databricks.spark.xml\") \\\n",
    "                    .option(\"rowTag\", \"book\") \\\n",
    "                    .load(\"books.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- genre: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- publish_date: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_books.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+---------------+-----+------------+--------------------+\n",
      "|  _id|              author|         description|          genre|price|publish_date|               title|\n",
      "+-----+--------------------+--------------------+---------------+-----+------------+--------------------+\n",
      "|bk101|Gambardella, Matthew|\n",
      "\n",
      "\n",
      "         An in...|       Computer|44.95|  2000-10-01|XML Developer's G...|\n",
      "|bk102|          Ralls, Kim|A former architec...|        Fantasy| 5.95|  2000-12-16|       Midnight Rain|\n",
      "|bk103|         Corets, Eva|After the collaps...|        Fantasy| 5.95|  2000-11-17|     Maeve Ascendant|\n",
      "|bk104|         Corets, Eva|In post-apocalyps...|        Fantasy| 5.95|  2001-03-10|     Oberon's Legacy|\n",
      "|bk105|         Corets, Eva|The two daughters...|        Fantasy| 5.95|  2001-09-10|  The Sundered Grail|\n",
      "|bk106|    Randall, Cynthia|When Carla meets ...|        Romance| 4.95|  2000-09-02|         Lover Birds|\n",
      "|bk107|      Thurman, Paula|A deep sea diver ...|        Romance| 4.95|  2000-11-02|       Splish Splash|\n",
      "|bk108|       Knorr, Stefan|An anthology of h...|         Horror| 4.95|  2000-12-06|     Creepy Crawlies|\n",
      "|bk109|        Kress, Peter|After an inadvert...|Science Fiction| 6.95|  2000-11-02|        Paradox Lost|\n",
      "|bk110|        O'Brien, Tim|Microsoft's .NET ...|       Computer|36.95|  2000-12-09|Microsoft .NET: T...|\n",
      "|bk111|        O'Brien, Tim|The Microsoft MSX...|       Computer|36.95|  2000-12-01|MSXML3: A Compreh...|\n",
      "|bk112|         Galos, Mike|Microsoft Visual ...|       Computer|49.95|  2001-04-16|Visual Studio 7: ...|\n",
      "+-----+--------------------+--------------------+---------------+-----+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_books.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf newbooks.xml\n",
    "\n",
    "selectedData = df_books.select(\"author\", \"_id\")\n",
    "selectedData.write.format(\"com.databricks.spark.xml\") \\\n",
    "                .option(\"rootTag\", \"books\") \\\n",
    "                .option(\"rowTag\", \"book\") \\\n",
    "                .save(\"newbooks.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "customSchema = StructType([StructField(\"_id\", StringType(), nullable = True), \n",
    "                           StructField(\"author\", StringType(), nullable = True),\n",
    "                           StructField(\"description\", StringType(), nullable = True),\n",
    "                           StructField(\"genre\", StringType(),nullable = True), \n",
    "                           StructField(\"price\", DoubleType(), nullable = True),\n",
    "                           StructField(\"publish_date\", StringType(), nullable = True),\n",
    "                           StructField(\"title\", StringType(), nullable = True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_books = sqlc.read.format(\"com.databricks.spark.xml\") \\\n",
    "                    .option(\"rowTag\", \"book\") \\\n",
    "                    .schema(customSchema) \\\n",
    "                    .load(\"books.xml\")\n",
    "            \n",
    "selectedData = df_books.select(\"author\", \"_id\")\n",
    "selectedData.write.format(\"com.databricks.spark.xml\") \\\n",
    "                .option(\"rootTag\", \"books\") \\\n",
    "                .option(\"rowTag\", \"book\") \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .save(\"newbooks.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7fa148b867d0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf metastore_db/*.lck\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+----------+-------------+\n",
      "| id|account|year|commission|   sales_reps|\n",
      "+---+-------+----+----------+-------------+\n",
      "|  1|   Acme|2013|      1000|   [Jim, Tom]|\n",
      "|  2|  Lumos|2013|      1100|  [Fred, Ann]|\n",
      "|  3|   Acme|2014|      2800|        [Jim]|\n",
      "|  4|  Lumos|2014|      1200|        [Ann]|\n",
      "|  5|   Acme|2014|      4200|[Fred, Sally]|\n",
      "+---+-------+----+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "Sales = namedtuple(\"Sales\",[\"id\",\"account\",\"year\",\"commission\",\"sales_reps\"])\n",
    "\n",
    "sales = sc.parallelize([Sales(1, \"Acme\", \"2013\", 1000, [\"Jim\", \"Tom\"]),\n",
    "         Sales(2, \"Lumos\", \"2013\", 1100, [\"Fred\", \"Ann\"]),\n",
    "         Sales(3, \"Acme\", \"2014\", 2800, [\"Jim\"]),\n",
    "         Sales(4, \"Lumos\", \"2014\", 1200, [\"Ann\"]),\n",
    "         Sales(5, \"Acme\", \"2014\", 4200, [\"Fred\", \"Sally\"])]).toDF()\n",
    "\n",
    "sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+----------+---------+\n",
      "| id|account|year|commission|sales_rep|\n",
      "+---+-------+----+----------+---------+\n",
      "|  1|   Acme|2013|      1000|      Jim|\n",
      "|  1|   Acme|2013|      1000|      Tom|\n",
      "|  2|  Lumos|2013|      1100|     Fred|\n",
      "|  2|  Lumos|2013|      1100|      Ann|\n",
      "|  3|   Acme|2014|      2800|      Jim|\n",
      "|  4|  Lumos|2014|      1200|      Ann|\n",
      "|  5|   Acme|2014|      4200|     Fred|\n",
      "|  5|   Acme|2014|      4200|    Sally|\n",
      "+---+-------+----+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "sales.select(\"id\",\"account\",\"year\",\"commission\",explode(\"sales_reps\").alias(\"sales_rep\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+----------+--------+---------+\n",
      "| id|account|year|commission|num_reps|sales_rep|\n",
      "+---+-------+----+----------+--------+---------+\n",
      "|  1|   Acme|2013|      1000|       2|      Jim|\n",
      "|  1|   Acme|2013|      1000|       2|      Tom|\n",
      "|  2|  Lumos|2013|      1100|       2|     Fred|\n",
      "|  2|  Lumos|2013|      1100|       2|      Ann|\n",
      "|  3|   Acme|2014|      2800|       1|      Jim|\n",
      "|  4|  Lumos|2014|      1200|       1|      Ann|\n",
      "|  5|   Acme|2014|      4200|       2|     Fred|\n",
      "|  5|   Acme|2014|      4200|       2|    Sally|\n",
      "+---+-------+----+----------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "column_len = UserDefinedFunction(lambda x: len(x), IntegerType())\n",
    "\n",
    "exploded = sales.select(\"id\",\"account\",\"year\",\"commission\",\n",
    "             column_len(\"sales_reps\").alias(\"num_reps\"),\n",
    "             explode(\"sales_reps\").alias(\"sales_rep\"))\n",
    "\n",
    "exploded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+----------+---------+------+\n",
      "| id|account|year|commission|sales_rep| share|\n",
      "+---+-------+----+----------+---------+------+\n",
      "|  1|   Acme|2013|      1000|      Jim| 500.0|\n",
      "|  1|   Acme|2013|      1000|      Tom| 500.0|\n",
      "|  2|  Lumos|2013|      1100|     Fred| 550.0|\n",
      "|  2|  Lumos|2013|      1100|      Ann| 550.0|\n",
      "|  3|   Acme|2014|      2800|      Jim|2800.0|\n",
      "|  4|  Lumos|2014|      1200|      Ann|1200.0|\n",
      "|  5|   Acme|2014|      4200|     Fred|2100.0|\n",
      "|  5|   Acme|2014|      4200|    Sally|2100.0|\n",
      "+---+-------+----+----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exploded = exploded.withColumn(\"share\", exploded.commission / exploded.num_reps).drop(\"num_reps\")\n",
    "exploded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------+\n",
      "|sales_rep| 2013|  2014|\n",
      "+---------+-----+------+\n",
      "|      Ann|550.0|1200.0|\n",
      "|     Fred|550.0|2100.0|\n",
      "|      Jim|500.0|2800.0|\n",
      "|    Sally| null|2100.0|\n",
      "|      Tom|500.0|  null|\n",
      "+---------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exploded.groupBy(\"sales_rep\").pivot(\"year\").sum(\"share\").orderBy(\"sales_rep\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----+------+\n",
      "|account|sales_rep| 2013|  2014|\n",
      "+-------+---------+-----+------+\n",
      "|   Acme|     Fred| null|2100.0|\n",
      "|   Acme|      Jim|500.0|2800.0|\n",
      "|   Acme|    Sally| null|2100.0|\n",
      "|   Acme|      Tom|500.0|  null|\n",
      "|  Lumos|      Ann|550.0|1200.0|\n",
      "|  Lumos|     Fred|550.0|  null|\n",
      "+-------+---------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exploded.groupBy(\"account\", \"sales_rep\").pivot(\"year\").sum(\"share\").orderBy(\"account\", \"sales_rep\").show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
