{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7feee903f7d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf metastore_db/*.lck\n",
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Vector Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Customer = namedtuple('Customer', ['churn','sessions','revenue','recency'])\n",
    "\n",
    "customers = sc.parallelize([Customer(1, 20, 61.24, 103),\n",
    "                            Customer(1, 8, 80.64, 23),\n",
    "                            Customer(0, 4, 100.94, 42),\n",
    "                            Customer(0, 8, 99.48, 26),\n",
    "                            Customer(1, 17, 120.56, 47)]).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler().setInputCols([\"sessions\", \"revenue\", \"recency\"]).setOutputCol(\"features\")\n",
    "dfWithFeatures = assembler.transform(customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------+-------+------------------+\n",
      "|churn|sessions|revenue|recency|          features|\n",
      "+-----+--------+-------+-------+------------------+\n",
      "|    1|      20|  61.24|    103|[20.0,61.24,103.0]|\n",
      "|    1|       8|  80.64|     23|  [8.0,80.64,23.0]|\n",
      "|    0|       4| 100.94|     42| [4.0,100.94,42.0]|\n",
      "|    0|       8|  99.48|     26|  [8.0,99.48,26.0]|\n",
      "|    1|      17| 120.56|     47|[17.0,120.56,47.0]|\n",
      "+-----+--------+-------+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithFeatures.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vector Slicer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorSlicer\n",
    "slicer = VectorSlicer().setInputCol(\"features\").setOutputCol(\"some_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------+-------+------------------+-------------+\n",
      "|churn|sessions|revenue|recency|          features|some_features|\n",
      "+-----+--------+-------+-------+------------------+-------------+\n",
      "|    1|      20|  61.24|    103|[20.0,61.24,103.0]| [20.0,61.24]|\n",
      "|    1|       8|  80.64|     23|  [8.0,80.64,23.0]|  [8.0,80.64]|\n",
      "|    0|       4| 100.94|     42| [4.0,100.94,42.0]| [4.0,100.94]|\n",
      "|    0|       8|  99.48|     26|  [8.0,99.48,26.0]|  [8.0,99.48]|\n",
      "|    1|      17| 120.56|     47|[17.0,120.56,47.0]|[17.0,120.56]|\n",
      "+-----+--------+-------+-------+------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "slicer.setIndices([0, 1]).transform(dfWithFeatures).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7feee903f7d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf metastore_db/*.lck\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df = sqlc.createDataFrame([(0, \"US\"), (1, \"UK\"), (2, \"FR\"),(3, \"US\"), (4, \"US\"), (5, \"FR\")]).toDF(\"id\", \"nationality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### String Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer().setInputCol(\"nationality\").setOutputCol(\"nIndex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "indexed = indexer.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------+\n",
      "| id|nationality|nIndex|\n",
      "+---+-----------+------+\n",
      "|  0|         US|   0.0|\n",
      "|  1|         UK|   2.0|\n",
      "|  2|         FR|   1.0|\n",
      "|  3|         US|   0.0|\n",
      "|  4|         US|   0.0|\n",
      "|  5|         FR|   1.0|\n",
      "+---+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### IndexToString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IndexToString\n",
    "\n",
    "converter = IndexToString().setInputCol(\"predictedIndex\").setOutputCol(\"predictedNationality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "predictions = indexed.selectExpr(\"nIndex as predictedIndex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|predictedIndex|predictedNationality|\n",
      "+--------------+--------------------+\n",
      "|           0.0|                  US|\n",
      "|           2.0|                  UK|\n",
      "|           1.0|                  FR|\n",
      "|           0.0|                  US|\n",
      "|           0.0|                  US|\n",
      "|           1.0|                  FR|\n",
      "+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "converter.transform(predictions).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder().setInputCol(\"nIndex\").setOutputCol(\"nVector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "encoded = encoder.transform(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------+-------------+\n",
      "| id|nationality|nIndex|      nVector|\n",
      "+---+-----------+------+-------------+\n",
      "|  0|         US|   0.0|(2,[0],[1.0])|\n",
      "|  1|         UK|   2.0|    (2,[],[])|\n",
      "|  2|         FR|   1.0|(2,[1],[1.0])|\n",
      "|  3|         US|   0.0|(2,[0],[1.0])|\n",
      "|  4|         US|   0.0|(2,[0],[1.0])|\n",
      "|  5|         FR|   1.0|(2,[1],[1.0])|\n",
      "+---+-----------+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder().setInputCol(\"nIndex\").setOutputCol(\"nVector\").setDropLast(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "encoded = encoder.transform(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------+-------------+\n",
      "| id|nationality|nIndex|      nVector|\n",
      "+---+-----------+------+-------------+\n",
      "|  0|         US|   0.0|(3,[0],[1.0])|\n",
      "|  1|         UK|   2.0|(3,[2],[1.0])|\n",
      "|  2|         FR|   1.0|(3,[1],[1.0])|\n",
      "|  3|         US|   0.0|(3,[0],[1.0])|\n",
      "|  4|         US|   0.0|(3,[0],[1.0])|\n",
      "|  5|         FR|   1.0|(3,[1],[1.0])|\n",
      "+---+-----------+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7feee903f7d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf metastore_db/*.lck\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-01-24 12:39:53--  https://s3.eu-central-1.amazonaws.com/dsr-data/UScrime/UScrime2-colsLotsOfNAremoved.csv\n",
      "Resolving s3.eu-central-1.amazonaws.com (s3.eu-central-1.amazonaws.com)... 54.231.193.41\n",
      "Connecting to s3.eu-central-1.amazonaws.com (s3.eu-central-1.amazonaws.com)|54.231.193.41|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 971758 (949K) [application/octet-stream]\n",
      "Saving to: ‘UScrime2-colsLotsOfNAremoved.csv’\n",
      "\n",
      "UScrime2-colsLotsOf 100%[===================>] 948,98K  1,91MB/s    in 0,5s    \n",
      "\n",
      "2017-01-24 12:39:53 (1,91 MB/s) - ‘UScrime2-colsLotsOfNAremoved.csv’ saved [971758/971758]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.eu-central-1.amazonaws.com/dsr-data/UScrime/UScrime2-colsLotsOfNAremoved.csv\n",
    "\n",
    "crimes = sqlc.read.format(\"com.databricks.spark.csv\") \\\n",
    "            .option(\"delimiter\", \",\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .load(\"UScrime2-colsLotsOfNAremoved.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "crimes = crimes.drop(\"OtherPerCap\").drop(\"community\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors, Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler().setInputCols(crimes.columns).setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "featuresDF = assembler.transform(crimes).select(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>pca</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.19, 0.33, 0.02, 0.9, 0.12, 0.17, 0.34, 0.47...</td>\n",
       "      <td>[1.2138889197, 0.564567759337, -0.022284837106...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0, 0.16, 0.12, 0.74, 0.45, 0.07, 0.26, 0.59...</td>\n",
       "      <td>[0.627985190195, 1.16689414866, -0.51416430664...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0, 0.42, 0.49, 0.56, 0.17, 0.04, 0.39, 0.47...</td>\n",
       "      <td>[0.234349043189, 0.348070144228, 0.54876884160...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            features  \\\n",
       "0  [0.19, 0.33, 0.02, 0.9, 0.12, 0.17, 0.34, 0.47...   \n",
       "1  [0.0, 0.16, 0.12, 0.74, 0.45, 0.07, 0.26, 0.59...   \n",
       "2  [0.0, 0.42, 0.49, 0.56, 0.17, 0.04, 0.39, 0.47...   \n",
       "\n",
       "                                                 pca  \n",
       "0  [1.2138889197, 0.564567759337, -0.022284837106...  \n",
       "1  [0.627985190195, 1.16689414866, -0.51416430664...  \n",
       "2  [0.234349043189, 0.348070144228, 0.54876884160...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "pca = PCA(k=10, inputCol=\"features\", outputCol=\"pca\")\n",
    "\n",
    "model = pca.fit(featuresDF)\n",
    "\n",
    "pc = model.transform(featuresDF)\n",
    "\n",
    "pc.toPandas()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7feee903f7d0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf metastore_db/*.lck\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## R Formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "crimes = sqlc.read.format(\"com.databricks.spark.csv\") \\\n",
    "            .option(\"delimiter\", \",\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .load(\"UScrime2-colsLotsOfNAremoved.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "formula = RFormula().setFormula(\" ViolentCrimesPerPop ~ householdsize + racepctblack + racePctWhite \") \\\n",
    "                    .setFeaturesCol(\"features\") \\\n",
    "                    .setLabelCol(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "output = formula.fit(crimes).transform(crimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|        features|label|\n",
      "+----------------+-----+\n",
      "| [0.33,0.02,0.9]|  0.2|\n",
      "|[0.16,0.12,0.74]| 0.67|\n",
      "|[0.42,0.49,0.56]| 0.43|\n",
      "+----------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.select(\"features\", \"label\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
