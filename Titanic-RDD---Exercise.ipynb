{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f65ed533790>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-10-01 16:33:43--  https://raw.githubusercontent.com/6chaoran/DataStory/master/Titanic-Spark/train.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.12.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.12.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 61194 (60K) [text/plain]\n",
      "Saving to: ‘train.csv.1’\n",
      "\n",
      "100%[======================================>] 61.194      --.-K/s   in 0,06s   \n",
      "\n",
      "2016-10-01 16:33:43 (959 KB/s) - ‘train.csv.1’ saved [61194/61194]\n",
      "\n",
      "--2016-10-01 16:33:43--  https://raw.githubusercontent.com/6chaoran/DataStory/master/Titanic-Spark/test.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.12.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.12.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 28629 (28K) [text/plain]\n",
      "Saving to: ‘test.csv.1’\n",
      "\n",
      "100%[======================================>] 28.629      --.-K/s   in 0,03s   \n",
      "\n",
      "2016-10-01 16:33:43 (915 KB/s) - ‘test.csv.1’ saved [28629/28629]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/6chaoran/DataStory/master/Titanic-Spark/train.csv\n",
    "!wget https://raw.githubusercontent.com/6chaoran/DataStory/master/Titanic-Spark/test.csv\n",
    "\n",
    "data = sc.textFile('./Titanic/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked',\n",
       " u'1,0,3,\"Braund, Mr. Owen Harris\",male,22,1,0,A/5 21171,7.25,,S',\n",
       " u'2,1,1,\"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\",female,38,1,0,PC 17599,71.2833,C85,C',\n",
       " u'3,1,3,\"Heikkinen, Miss. Laina\",female,26,0,0,STON/O2. 3101282,7.925,,S',\n",
       " u'4,1,1,\"Futrelle, Mrs. Jacques Heath (Lily May Peel)\",female,35,1,0,113803,53.1,C123,S']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "- Split the fields and make sure to not include the header in the dataset itself.\n",
    "- Note some variables are categorical - use some transformation to encode them as numeric values\n",
    "- You may choose NOT to use all variables\n",
    "- Assembler all variables you chose to keep as a Dense Vector, so you finish this step with an RDD[Vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors, Vector\n",
    "from StringIO import StringIO\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "## INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "- Explore the features you have assembled\n",
    "- You can use RDD operations as ***map*** and ***countByKey*** to check how many different values a given feature has and also its distribution\n",
    "- You can use ***Statistics*** to obtain statistics regarding your features - for instance, the corresponding means\n",
    "- Are there any ***NaN*** values in your dataset?\n",
    "- If so, define value/values to fill these ***NaN*** values\n",
    "    - hint: you can decompose the Dense vector using its ***values*** property and turning it into a list, change the desired value and reassemble it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "## INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "- Make your RDD an RDD[LabeledPoint]\n",
    "- Train a classifier of your choice (for instance, Random Forest) using your dataset of LabeledPoints\n",
    "- Make predictions for the training data\n",
    "- Use the Binary Classification Metrics to evaluate your model on the training data\n",
    "- How is your model performing? Try to tune its parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import LabeledPoint\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "- Apply the data processing/transforming steps to the testing data\n",
    "    - hint: it could be useful to revisit your past code and refactor it as function(s) taking an RDD as parameter and performing the operations on it\n",
    "- Make predictions for the test data\n",
    "- Save it as ***submission.csv*** and submit it to Kaggle\n",
    "- What was your score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(zip(range(892,1310), \n",
    "                 <YOUR PREDICTIONS RDD>.map(int).collect()), \n",
    "             columns=['PassengerId','Survived']).to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result = ???%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
