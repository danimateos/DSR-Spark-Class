{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f3e5d740710>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf metastore_db/*.lck\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-02-07 14:42:52--  https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_libsvm_data.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.112.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.112.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 104736 (102K) [text/plain]\n",
      "Saving to: ‘sample_libsvm_data.txt.1’\n",
      "\n",
      "sample_libsvm_data. 100%[===================>] 102,28K  --.-KB/s    in 0,1s    \n",
      "\n",
      "2017-02-07 14:42:52 (1,04 MB/s) - ‘sample_libsvm_data.txt.1’ saved [104736/104736]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "!wget https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_libsvm_data.txt\n",
    "\n",
    "data = MLUtils.loadLibSVMFile(sc, \"sample_libsvm_data.txt\").toDF()\n",
    "data = MLUtils.convertVectorColumnsToML(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(692,[127,128,129...|  0.0|\n",
      "|(692,[158,159,160...|  1.0|\n",
      "|(692,[124,125,126...|  1.0|\n",
      "|(692,[152,153,154...|  1.0|\n",
      "|(692,[151,152,153...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString, VectorIndexer\n",
    "\n",
    "labelIndexer = StringIndexer().setInputCol(\"label\").setOutputCol(\"indexedLabel\").fit(data)\n",
    "\n",
    "labelConverter = IndexToString().setInputCol(\"prediction\").setOutputCol(\"predictedLabel\").setLabels(labelIndexer.labels)\n",
    "\n",
    "featureIndexer = VectorIndexer().setInputCol(\"features\").setOutputCol(\"indexedFeatures\").setMaxCategories(4).fit(data)\n",
    "\n",
    "dtC = DecisionTreeClassifier().setLabelCol(\"indexedLabel\").setFeaturesCol(\"indexedFeatures\")\n",
    "\n",
    "pipelineClass = Pipeline().setStages([labelIndexer, featureIndexer, dtC, labelConverter])\n",
    "\n",
    "trainingData, testData = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier, DecisionTreeClassificationModel\n",
    "\n",
    "modelClassifier = pipelineClass.fit(trainingData)\n",
    "\n",
    "treeModel = modelClassifier.stages[2]\n",
    "\n",
    "predictionsClass = modelClassifier.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_4060b8fbdaa01e6e664f,\n",
       " VectorIndexer_49cc9b0e69d1b82d4dad,\n",
       " DecisionTreeClassificationModel (uid=DecisionTreeClassifier_4e88a70d392f75c207fe) of depth 2 with 5 nodes,\n",
       " IndexToString_41c69abf49da405a7ef1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelClassifier.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_4e88a70d392f75c207fe) of depth 2 with 5 nodes\n",
      "  If (feature 406 <= 72.0)\n",
      "   If (feature 99 in {2.0})\n",
      "    Predict: 0.0\n",
      "   Else (feature 99 not in {2.0})\n",
      "    Predict: 1.0\n",
      "  Else (feature 406 > 72.0)\n",
      "   Predict: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print treeModel.toDebugString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>label</th>\n",
       "      <th>indexedLabel</th>\n",
       "      <th>indexedFeatures</th>\n",
       "      <th>rawPrediction</th>\n",
       "      <th>probability</th>\n",
       "      <th>prediction</th>\n",
       "      <th>predictedLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[35.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 29.0]</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[35.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[35.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 29.0]</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            features  label  indexedLabel  \\\n",
       "0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    1.0           0.0   \n",
       "1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    0.0           1.0   \n",
       "2  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    1.0           0.0   \n",
       "3  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    1.0           0.0   \n",
       "4  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    0.0           1.0   \n",
       "\n",
       "                                     indexedFeatures rawPrediction  \\\n",
       "0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   [35.0, 0.0]   \n",
       "1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   [0.0, 29.0]   \n",
       "2  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   [35.0, 0.0]   \n",
       "3  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   [35.0, 0.0]   \n",
       "4  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   [0.0, 29.0]   \n",
       "\n",
       "  probability  prediction predictedLabel  \n",
       "0  [1.0, 0.0]         0.0            1.0  \n",
       "1  [0.0, 1.0]         1.0            0.0  \n",
       "2  [1.0, 0.0]         0.0            1.0  \n",
       "3  [1.0, 0.0]         0.0            1.0  \n",
       "4  [0.0, 1.0]         1.0            0.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionsClass.toPandas()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor, DecisionTreeRegressionModel\n",
    "\n",
    "dtR = DecisionTreeRegressor().setLabelCol(\"label\").setFeaturesCol(\"indexedFeatures\")\n",
    "\n",
    "pipelineReg = Pipeline().setStages([featureIndexer, dtR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeRegressionModel (uid=DecisionTreeRegressor_4220915f1fe28972014b) of depth 2 with 5 nodes\n",
      "  If (feature 406 <= 72.0)\n",
      "   If (feature 99 in {0.0,3.0})\n",
      "    Predict: 0.0\n",
      "   Else (feature 99 not in {0.0,3.0})\n",
      "    Predict: 1.0\n",
      "  Else (feature 406 > 72.0)\n",
      "   Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelRegressor = pipelineReg.fit(trainingData)\n",
    "\n",
    "treeModel = modelRegressor.stages[1]\n",
    "\n",
    "print treeModel.toDebugString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "predictionsReg = modelRegressor.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>label</th>\n",
       "      <th>indexedFeatures</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            features  label  \\\n",
       "0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    1.0   \n",
       "1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    0.0   \n",
       "2  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    1.0   \n",
       "3  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    1.0   \n",
       "4  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    0.0   \n",
       "\n",
       "                                     indexedFeatures  prediction  \n",
       "0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         1.0  \n",
       "1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         0.0  \n",
       "2  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         1.0  \n",
       "3  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         1.0  \n",
       "4  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         0.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionsReg.toPandas()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f3e5d740710>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf metastore_db/*.lck\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "data = MLUtils.loadLibSVMFile(sc, \"sample_libsvm_data.txt\").toDF()\n",
    "data = MLUtils.convertVectorColumnsToML(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(692,[127,128,129...|  0.0|\n",
      "|(692,[158,159,160...|  1.0|\n",
      "|(692,[124,125,126...|  1.0|\n",
      "|(692,[152,153,154...|  1.0|\n",
      "|(692,[151,152,153...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString, VectorIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "\n",
    "labelIndexer = StringIndexer().setInputCol(\"label\") \\\n",
    "                                .setOutputCol(\"indexedLabel\").fit(data)\n",
    "\n",
    "labelConverter = IndexToString().setInputCol(\"prediction\") \\\n",
    "                                .setOutputCol(\"predictedLabel\") \\\n",
    "                                .setLabels(labelIndexer.labels)\n",
    "\n",
    "featureIndexer = VectorIndexer().setInputCol(\"features\") \\\n",
    "                                .setOutputCol(\"indexedFeatures\") \\\n",
    "                                .setMaxCategories(4).fit(data)\n",
    "\n",
    "rfC = RandomForestClassifier().setLabelCol(\"indexedLabel\") \\\n",
    "                                .setFeaturesCol(\"indexedFeatures\") \\\n",
    "                                .setNumTrees(3)\n",
    "        \n",
    "trainingData, testData = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipelineRFC = Pipeline().setStages([labelIndexer, featureIndexer, rfC, labelConverter])\n",
    "\n",
    "modelRFC = pipelineRFC.fit(trainingData)\n",
    "\n",
    "predictionsRFC = modelRFC.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+--------------------+\n",
      "|predictedLabel|label|            features|\n",
      "+--------------+-----+--------------------+\n",
      "|           1.0|  1.0|(692,[97,98,99,12...|\n",
      "|           0.0|  0.0|(692,[98,99,100,1...|\n",
      "|           1.0|  1.0|(692,[99,100,101,...|\n",
      "|           0.0|  0.0|(692,[122,123,148...|\n",
      "|           0.0|  0.0|(692,[126,127,128...|\n",
      "+--------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionsRFC.select(\"predictedLabel\", \"label\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(692, {329: 0.0373, 455: 0.2992, 457: 0.0175, 490: 0.3333, 538: 0.2961, 540: 0.0166})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfModelC = modelRFC.stages[2]\n",
    "rfModelC.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassificationModel (uid=rfc_1d2da6331491) with 3 trees\n",
      "  Tree 0 (weight 1.0):\n",
      "    If (feature 455 <= 0.0)\n",
      "     If (feature 540 <= 65.0)\n",
      "      If (feature 457 <= 0.0)\n",
      "       Predict: 0.0\n",
      "      Else (feature 457 > 0.0)\n",
      "       Predict: 1.0\n",
      "     Else (feature 540 > 65.0)\n",
      "      Predict: 1.0\n",
      "    Else (feature 455 > 0.0)\n",
      "     Predict: 1.0\n",
      "  Tree 1 (weight 1.0):\n",
      "    If (feature 490 <= 0.0)\n",
      "     Predict: 1.0\n",
      "    Else (feature 490 > 0.0)\n",
      "     Predict: 0.0\n",
      "  Tree 2 (weight 1.0):\n",
      "    If (feature 538 <= 0.0)\n",
      "     If (feature 329 <= 0.0)\n",
      "      Predict: 0.0\n",
      "     Else (feature 329 > 0.0)\n",
      "      Predict: 1.0\n",
      "    Else (feature 538 > 0.0)\n",
      "     Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print rfModelC.toDebugString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import RandomForestRegressionModel\n",
    "\n",
    "rfR = RandomForestRegressor().setLabelCol(\"label\").setFeaturesCol(\"indexedFeatures\")\n",
    "\n",
    "pipelineRFR = Pipeline().setStages([featureIndexer, rfR])\n",
    "\n",
    "modelRFR = pipelineRFR.fit(trainingData)\n",
    "\n",
    "predictionsRFR = modelRFR.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       1.0|  1.0|(692,[97,98,99,12...|\n",
      "|       0.0|  0.0|(692,[98,99,100,1...|\n",
      "|       0.1|  1.0|(692,[99,100,101,...|\n",
      "|       0.0|  0.0|(692,[122,123,148...|\n",
      "|       0.0|  0.0|(692,[126,127,128...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionsRFR = modelRFR.transform(testData)\n",
    "\n",
    "predictionsRFR.select(\"prediction\", \"label\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f3e5d740710>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf metastore_db/*.lck\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "data = MLUtils.loadLibSVMFile(sc, \"sample_libsvm_data.txt\").toDF()\n",
    "data = MLUtils.convertVectorColumnsToML(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(692,[127,128,129...|  0.0|\n",
      "|(692,[158,159,160...|  1.0|\n",
      "|(692,[124,125,126...|  1.0|\n",
      "|(692,[152,153,154...|  1.0|\n",
      "|(692,[151,152,153...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString, VectorIndexer\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import GBTClassificationModel\n",
    "\n",
    "labelIndexer = StringIndexer().setInputCol(\"label\") \\\n",
    "                                .setOutputCol(\"indexedLabel\").fit(data)\n",
    "\n",
    "labelConverter = IndexToString().setInputCol(\"prediction\") \\\n",
    "                                .setOutputCol(\"predictedLabel\") \\\n",
    "                                .setLabels(labelIndexer.labels)\n",
    "\n",
    "featureIndexer = VectorIndexer().setInputCol(\"features\") \\\n",
    "                                .setOutputCol(\"indexedFeatures\") \\\n",
    "                                .setMaxCategories(4).fit(data)\n",
    "\n",
    "gbtC = GBTClassifier().setLabelCol(\"indexedLabel\").setFeaturesCol(\"indexedFeatures\").setMaxIter(10)\n",
    "\n",
    "pipelineGBTC = Pipeline().setStages([labelIndexer, featureIndexer, gbtC, labelConverter])\n",
    "\n",
    "trainingData, testData = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+--------------------+\n",
      "|predictedLabel|label|            features|\n",
      "+--------------+-----+--------------------+\n",
      "|           0.0|  1.0|(692,[99,100,101,...|\n",
      "|           1.0|  1.0|(692,[119,120,121...|\n",
      "|           1.0|  1.0|(692,[123,124,125...|\n",
      "+--------------+-----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelGBTC = pipelineGBTC.fit(trainingData)\n",
    "\n",
    "predictionsGBTC = modelGBTC.transform(testData)\n",
    "\n",
    "predictionsGBTC.select(\"predictedLabel\", \"label\", \"features\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBTClassificationModel (uid=GBTClassifier_4fb78cb5d8561d32efe0) with 10 trees\n",
      "  Tree 0 (weight 1.0):\n",
      "    If (feature 434 <= 0.0)\n",
      "     Predict: 1.0\n",
      "    Else (feature 434 > 0.0)\n",
      "     Predict: -1.0\n",
      "  Tree 1 (weight 0.1):\n",
      "    If (feature 462 <= 0.0)\n",
      "     Predict: 0.47681168808847024\n",
      "    Else (feature 462 > 0.0)\n",
      "     If (feature 409 <= 25.0)\n",
      "      Predict: -0.47681168808847024\n",
      "     Else (feature 409 > 25.0)\n",
      "      Predict: -0.4768116880884712\n",
      "  Tree 2 (weight 0.1):\n",
      "    If (feature 434 <= 0.0)\n",
      "     If (feature 186 <= 15.0)\n",
      "      Predict: 0.4381935810427206\n",
      "     Else (feature 186 > 15.0)\n",
      "      Predict: 0.43819358104272066\n",
      "    Else (feature 434 > 0.0)\n",
      "     If (feature 437 <= 15.0)\n",
      "      Predict: -0.4381935810427206\n",
      "     Else (feature 437 > 15.0)\n",
      "      Predict: -0.43819358104272155\n",
      "  Tree 3 (weight 0.1):\n",
      "    If (feature 490 <= 0.0)\n",
      "     Predict: 0.40514968028459825\n",
      "    Else (feature 490 > 0.0)\n",
      "     If (feature 459 <= 189.0)\n",
      "      If (feature 459 <= 64.0)\n",
      "       If (feature 266 <= 0.0)\n",
      "        If (feature 185 <= 246.0)\n",
      "         Predict: -0.4051496802845983\n",
      "        Else (feature 185 > 246.0)\n",
      "         Predict: -0.40514968028459836\n",
      "       Else (feature 266 > 0.0)\n",
      "        Predict: -0.40514968028459836\n",
      "      Else (feature 459 > 64.0)\n",
      "       Predict: -0.4051496802845982\n",
      "     Else (feature 459 > 189.0)\n",
      "      Predict: -0.4051496802845988\n",
      "  Tree 4 (weight 0.1):\n",
      "    If (feature 490 <= 0.0)\n",
      "     If (feature 209 <= 34.0)\n",
      "      Predict: 0.3765841318352991\n",
      "     Else (feature 209 > 34.0)\n",
      "      Predict: 0.3765841318352992\n",
      "    Else (feature 490 > 0.0)\n",
      "     If (feature 657 <= 173.0)\n",
      "      Predict: -0.37658413183529915\n",
      "     Else (feature 657 > 173.0)\n",
      "      Predict: -0.3765841318352994\n",
      "  Tree 5 (weight 0.1):\n",
      "    If (feature 434 <= 0.0)\n",
      "     If (feature 214 <= 24.0)\n",
      "      Predict: 0.35166478958101005\n",
      "     Else (feature 214 > 24.0)\n",
      "      Predict: 0.3516647895810101\n",
      "    Else (feature 434 > 0.0)\n",
      "     Predict: -0.35166478958101\n",
      "  Tree 6 (weight 0.1):\n",
      "    If (feature 462 <= 0.0)\n",
      "     If (feature 127 <= 164.0)\n",
      "      Predict: 0.32974984655529926\n",
      "     Else (feature 127 > 164.0)\n",
      "      Predict: 0.32974984655529943\n",
      "    Else (feature 462 > 0.0)\n",
      "     Predict: -0.32974984655529926\n",
      "  Tree 7 (weight 0.1):\n",
      "    If (feature 434 <= 0.0)\n",
      "     If (feature 600 <= 106.0)\n",
      "      If (feature 179 <= 0.0)\n",
      "       Predict: 0.3103372455197956\n",
      "      Else (feature 179 > 0.0)\n",
      "       Predict: 0.3103372455197957\n",
      "     Else (feature 600 > 106.0)\n",
      "      Predict: 0.3103372455197957\n",
      "    Else (feature 434 > 0.0)\n",
      "     If (feature 545 <= 64.0)\n",
      "      If (feature 323 <= 252.0)\n",
      "       Predict: -0.3103372455197956\n",
      "      Else (feature 323 > 252.0)\n",
      "       Predict: -0.3103372455197957\n",
      "     Else (feature 545 > 64.0)\n",
      "      If (feature 351 <= 109.0)\n",
      "       If (feature 127 <= 0.0)\n",
      "        Predict: -0.3103372455197956\n",
      "       Else (feature 127 > 0.0)\n",
      "        Predict: -0.3103372455197957\n",
      "      Else (feature 351 > 109.0)\n",
      "       Predict: -0.3103372455197957\n",
      "  Tree 8 (weight 0.1):\n",
      "    If (feature 434 <= 0.0)\n",
      "     If (feature 210 <= 48.0)\n",
      "      Predict: 0.2930291649125433\n",
      "     Else (feature 210 > 48.0)\n",
      "      Predict: 0.2930291649125434\n",
      "    Else (feature 434 > 0.0)\n",
      "     If (feature 489 <= 64.0)\n",
      "      Predict: -0.2930291649125433\n",
      "     Else (feature 489 > 64.0)\n",
      "      Predict: -0.2930291649125434\n",
      "  Tree 9 (weight 0.1):\n",
      "    If (feature 490 <= 0.0)\n",
      "     If (feature 154 <= 105.0)\n",
      "      If (feature 154 <= 0.0)\n",
      "       Predict: 0.27750666438358246\n",
      "      Else (feature 154 > 0.0)\n",
      "       Predict: 0.2775066643835825\n",
      "     Else (feature 154 > 105.0)\n",
      "      Predict: 0.2775066643835826\n",
      "    Else (feature 490 > 0.0)\n",
      "     If (feature 545 <= 0.0)\n",
      "      Predict: -0.27750666438358246\n",
      "     Else (feature 545 > 0.0)\n",
      "      Predict: -0.2775066643835825\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gbtModelC = modelGBTC.stages[2]\n",
    "\n",
    "print gbtModelC.toDebugString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.regression import GBTRegressionModel\n",
    "\n",
    "gbtR = GBTRegressor().setLabelCol(\"label\").setFeaturesCol(\"indexedFeatures\").setMaxIter(10)\n",
    "\n",
    "pipelineGBTR = Pipeline().setStages([featureIndexer, gbtR])\n",
    "\n",
    "modelGBTR = pipelineGBTR.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+----------+\n",
      "|            features|label|     indexedFeatures|prediction|\n",
      "+--------------------+-----+--------------------+----------+\n",
      "|(692,[99,100,101,...|  1.0|(692,[99,100,101,...|       0.0|\n",
      "|(692,[119,120,121...|  1.0|(692,[119,120,121...|       1.0|\n",
      "|(692,[123,124,125...|  1.0|(692,[123,124,125...|       1.0|\n",
      "|(692,[125,126,127...|  1.0|(692,[125,126,127...|       1.0|\n",
      "|(692,[125,126,153...|  1.0|(692,[125,126,153...|       1.0|\n",
      "+--------------------+-----+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionsGBTR = modelGBTR.transform(testData)\n",
    "predictionsGBTR.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f3e5d740710>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf metastore_db/ \n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "data = MLUtils.loadLibSVMFile(sc, \"sample_libsvm_data.txt\").toDF()\n",
    "data = MLUtils.convertVectorColumnsToML(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(692,[127,128,129...|  0.0|\n",
      "|(692,[158,159,160...|  1.0|\n",
      "|(692,[124,125,126...|  1.0|\n",
      "|(692,[152,153,154...|  1.0|\n",
      "|(692,[151,152,153...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: (692,[235,263,272,300,356,378,405,406,407,433,434,461,462,484,489,490,496,511,512,539,540,568],[-0.000184340020759,-0.00052020775993,-6.538669629e-05,-5.18477807096e-05,-4.8157070928e-05,0.000419623480238,0.000184843519456,0.000928670263834,0.00019572960232,0.000611997139151,0.00091992892924,0.000396707675263,0.000457937869717,-6.05102100784e-06,0.000194054328354,4.72616986833e-05,-5.26929702748e-06,-0.000303136407921,-0.000260957969962,-0.000280680238859,-0.00124563037765,-0.00129370037932]) Intercept: 0.0484695213565\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "trainingData, testData = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "logr = LogisticRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "\n",
    "logrModel = logr.fit(trainingData)\n",
    "\n",
    "print \"Weights: %s Intercept: %s\" % (logrModel.coefficients, logrModel.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "trainingSummaryLR = logrModel.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n",
      "|FPR|                TPR|\n",
      "+---+-------------------+\n",
      "|0.0|                0.0|\n",
      "|0.0|0.02564102564102564|\n",
      "|0.0|0.05128205128205128|\n",
      "|0.0|0.07692307692307693|\n",
      "|0.0|0.10256410256410256|\n",
      "|0.0| 0.1282051282051282|\n",
      "|0.0|0.15384615384615385|\n",
      "|0.0| 0.1794871794871795|\n",
      "|0.0|0.20512820512820512|\n",
      "|0.0|0.23076923076923078|\n",
      "|0.0| 0.2564102564102564|\n",
      "|0.0|0.28205128205128205|\n",
      "|0.0| 0.3076923076923077|\n",
      "|0.0| 0.3333333333333333|\n",
      "|0.0|  0.358974358974359|\n",
      "|0.0|0.38461538461538464|\n",
      "|0.0|0.41025641025641024|\n",
      "|0.0| 0.4358974358974359|\n",
      "|0.0|0.46153846153846156|\n",
      "|0.0|0.48717948717948717|\n",
      "+---+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingSummaryLR.roc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingSummaryLR.areaUnderROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-4.40977414446e-06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.000129512650523,5.06077897729e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0,0.000184151125733,0.000124120258303,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.000157868005873,0.000219184304722,0.000163642440859,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.000210117910035,0.000216232532174,1.88262572339e-06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.000171548803633,0.000203990101763,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.000165490324512,0.000141425287566,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-5.05076673128e-06,-2.81458933931e-06,0.0,0.0,0.0,0.0,0.000132289862881,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-2.23222192774e-06,-1.21964735681e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-3.2642222123e-06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] Intercept: 0.258738141536\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "\n",
    "lrModel = lr.fit(trainingData)\n",
    "\n",
    "print \"Weights: %s Intercept: %s\" % (lrModel.coefficients, lrModel.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numIterations: 11\n"
     ]
    }
   ],
   "source": [
    "trainingSummaryLLS = lrModel.summary\n",
    "\n",
    "print \"numIterations: %s\" % trainingSummaryLLS.totalIterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "|0.34509718096855513|\n",
      "|-0.2524915778203864|\n",
      "|0.19613366690992318|\n",
      "| 0.2149702950954704|\n",
      "| 0.3148710175072822|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingSummaryLLS.residuals.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f3e5d740710>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf metastore_db/*.lck\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "data = MLUtils.loadLibSVMFile(sc, \"sample_libsvm_data.txt\").toDF()\n",
    "data = MLUtils.convertVectorColumnsToML(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(692,[127,128,129...|  0.0|\n",
      "|(692,[158,159,160...|  1.0|\n",
      "|(692,[124,125,126...|  1.0|\n",
      "|(692,[152,153,154...|  1.0|\n",
      "|(692,[151,152,153...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: (692,[243,244,262,263,272,290,300,323,350,351,377,378,379,400,405,406,407,428,433,434,455,456,461,462,483,484,489,490,496,511,512,517,539,540,545,568,605,606,632],[-0.000339050678432,-5.5366680824e-06,-9.62816266919e-06,-0.00027433697171,-0.000362645776348,-9.36657429677e-06,-0.000327002184676,0.000104553610923,0.000154237573607,0.000151196195048,9.45942057644e-05,0.000223749607342,0.00019929149022,-1.10459264548e-05,0.000161416795613,0.00114844242452,0.000231993831837,-5.18597860749e-05,0.000236426801222,0.000778332744722,-8.44411190964e-05,-8.60709153075e-05,0.00022403449151,0.000229668781053,-0.000406434046326,-6.72101135132e-05,0.000221462851066,0.000139115480739,-5.80851145937e-05,-0.000823552541309,-0.000350876710739,0.000233110528072,-0.000636265842505,-0.00127800472718,5.93584906113e-05,-0.000382629540403,-7.02092445983e-05,-7.82607115707e-05,-3.99466577477e-05]) Intercept: 0.128768478595\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import BinaryLogisticRegressionSummary\n",
    "\n",
    "trainingData, testData = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "logr = LogisticRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "\n",
    "logrModel = logr.fit(trainingData)\n",
    "\n",
    "print \"Weights: %s Intercept: %s\" % (logrModel.coefficients, logrModel.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "predictionsLogR = logrModel.transform(testData)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator().setLabelCol(\"label\") \\\n",
    "                            .setRawPredictionCol(\"rawPrediction\") \\\n",
    "                            .setMetricName(\"areaUnderROC\")\n",
    "\n",
    "roc = evaluator.evaluate(predictionsLogR)\n",
    "print roc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString, VectorIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "\n",
    "labelIndexer = StringIndexer().setInputCol(\"label\") \\\n",
    "                                .setOutputCol(\"indexedLabel\").fit(data)\n",
    "\n",
    "labelConverter = IndexToString().setInputCol(\"prediction\") \\\n",
    "                                .setOutputCol(\"predictedLabel\") \\\n",
    "                                .setLabels(labelIndexer.labels)\n",
    "\n",
    "featureIndexer = VectorIndexer().setInputCol(\"features\") \\\n",
    "                                .setOutputCol(\"indexedFeatures\") \\\n",
    "                                .setMaxCategories(4).fit(data)\n",
    "\n",
    "rfC = RandomForestClassifier().setLabelCol(\"indexedLabel\") \\\n",
    "                                .setFeaturesCol(\"indexedFeatures\") \\\n",
    "                                .setNumTrees(3)\n",
    "        \n",
    "pipelineRFC = Pipeline().setStages([labelIndexer, featureIndexer, rfC, labelConverter])\n",
    "\n",
    "modelRFC = pipelineRFC.fit(trainingData)\n",
    "\n",
    "predictionsRFC = modelRFC.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator().setLabelCol(\"indexedLabel\") \\\n",
    "                                        .setPredictionCol(\"prediction\") \\\n",
    "                                        .setMetricName(\"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictionsRFC)\n",
    "\n",
    "print \"Test Error = %s\" % (1.0 - accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import RandomForestRegressionModel\n",
    "\n",
    "rfR = RandomForestRegressor().setLabelCol(\"label\").setFeaturesCol(\"indexedFeatures\")\n",
    "\n",
    "pipelineRFR = Pipeline().setStages([featureIndexer, rfR])\n",
    "\n",
    "modelRFR = pipelineRFR.fit(trainingData)\n",
    "\n",
    "predictionsRFR = modelRFR.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) = 0.0869718492623\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator().setLabelCol(\"label\") \\\n",
    "                                .setPredictionCol(\"prediction\") \\\n",
    "                                .setMetricName(\"rmse\")\n",
    "\n",
    "rmse = evaluator.evaluate(predictionsRFR)\n",
    "\n",
    "print \"Root Mean Squared Error (RMSE) = %s\" % rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: (692,[243,244,262,263,272,290,300,323,350,351,377,378,379,400,405,406,407,428,433,434,455,456,461,462,483,484,489,490,496,511,512,517,539,540,545,568,605,606,632],[-0.000339050678432,-5.5366680824e-06,-9.62816266919e-06,-0.00027433697171,-0.000362645776348,-9.36657429677e-06,-0.000327002184676,0.000104553610923,0.000154237573607,0.000151196195048,9.45942057644e-05,0.000223749607342,0.00019929149022,-1.10459264548e-05,0.000161416795613,0.00114844242452,0.000231993831837,-5.18597860749e-05,0.000236426801222,0.000778332744722,-8.44411190964e-05,-8.60709153075e-05,0.00022403449151,0.000229668781053,-0.000406434046326,-6.72101135132e-05,0.000221462851066,0.000139115480739,-5.80851145937e-05,-0.000823552541309,-0.000350876710739,0.000233110528072,-0.000636265842505,-0.00127800472718,5.93584906113e-05,-0.000382629540403,-7.02092445983e-05,-7.82607115707e-05,-3.99466577477e-05]) Intercept: 0.128768478595\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "logr = LogisticRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "\n",
    "logrModel = logr.fit(trainingData)\n",
    "\n",
    "print \"Weights: %s Intercept: %s\" % (logrModel.coefficients, logrModel.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingSummaryLR = logrModel.summary\n",
    "trainingSummaryLR.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+\n",
      "|         threshold|          F-Measure|\n",
      "+------------------+-------------------+\n",
      "| 0.783800770148522|0.05882352941176471|\n",
      "|0.7830613181129249| 0.1142857142857143|\n",
      "|0.7812208460751318|0.16666666666666669|\n",
      "+------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fMeasure = trainingSummaryLR.fMeasureByThreshold\n",
    "\n",
    "fMeasure.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "0.550683014429\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "maxFMeasure = fMeasure.agg({\"F-Measure\": \"max\"}).head()[0]\n",
    "print maxFMeasure\n",
    "maxFMeasure = fMeasure.agg(F.max(F.col(\"F-Measure\"))).head()[0]\n",
    "print maxFMeasure\n",
    "\n",
    "bestThreshold = fMeasure.where(F.col(\"F-Measure\") == maxFMeasure).select(\"threshold\").head()[0]\n",
    "print bestThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|              recall|precision|\n",
      "+--------------------+---------+\n",
      "|                 0.0|      1.0|\n",
      "|0.030303030303030304|      1.0|\n",
      "| 0.06060606060606061|      1.0|\n",
      "+--------------------+---------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------------------+---------+\n",
      "|         threshold|precision|\n",
      "+------------------+---------+\n",
      "| 0.783800770148522|      1.0|\n",
      "|0.7830613181129249|      1.0|\n",
      "|0.7812208460751318|      1.0|\n",
      "+------------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingSummaryLR.pr.show(3)\n",
    "trainingSummaryLR.precisionByThreshold.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|         threshold|              recall|\n",
      "+------------------+--------------------+\n",
      "| 0.783800770148522|0.030303030303030304|\n",
      "|0.7830613181129249| 0.06060606060606061|\n",
      "|0.7812208460751318| 0.09090909090909091|\n",
      "+------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---+--------------------+\n",
      "|FPR|                 TPR|\n",
      "+---+--------------------+\n",
      "|0.0|                 0.0|\n",
      "|0.0|0.030303030303030304|\n",
      "|0.0| 0.06060606060606061|\n",
      "+---+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingSummaryLR.recallByThreshold.show(3)\n",
    "trainingSummaryLR.roc.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-6.24179431308e-06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-7.07104305837e-06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-1.32712972999e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-1.76666179551e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.25577440472e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.55424619313e-05,1.50216626734e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,9.35502041754e-06,0.000153015864969,0.000132838728228,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.000131024143317,0.000229918607219,0.000156054888207,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.00016130325678,0.000216434520018,7.7379967933e-06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.000151494813113,0.000155625994132,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-1.49186671606e-05,0.0,0.0,0.0,0.0,0.0,0.000149469215318,1.44866405997e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-1.72395499448e-05,-1.20190800906e-05,0.0,0.0,0.0,0.0,0.000157911104434,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-1.37300615538e-05,-0.00013256058484,0.0,0.0,0.0,0.0,8.83038057486e-06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-8.1010880323e-06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] Intercept: 0.331622539418\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "\n",
    "lrModel = lr.fit(trainingData)\n",
    "\n",
    "print \"Weights: %s Intercept: %s\" % (lrModel.coefficients, lrModel.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0563067869876\n",
      "0.26645263604\n",
      "0.0744390790341\n",
      "0.700229639518\n"
     ]
    }
   ],
   "source": [
    "trainingSummaryLLS = lrModel.summary\n",
    "\n",
    "print trainingSummaryLLS.explainedVariance\n",
    "\n",
    "print trainingSummaryLLS.meanAbsoluteError\n",
    "\n",
    "print trainingSummaryLLS.meanSquaredError\n",
    "\n",
    "print trainingSummaryLLS.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "|  0.252507432655962|\n",
      "| 0.5787310312766889|\n",
      "|-0.2733425522661087|\n",
      "+-------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "0.272835259881\n"
     ]
    }
   ],
   "source": [
    "trainingSummaryLLS.residuals.show(3)\n",
    "\n",
    "print trainingSummaryLLS.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f3e5d740710>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf metastore_db/*.lck\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Saving and Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "df = sqlc.createDataFrame([(1.0, Vectors.dense(1.0, 2.0, 3.0)),\n",
    "                           (1.0, Vectors.dense(2.0, 3.0, 4.0)),\n",
    "                           (0.0, Vectors.dense(-1.0, 1.0, 2.0)),\n",
    "                           (0.0, Vectors.dense(-2.0, 3.0, 5.0))]).toDF(\"label\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "|label|      features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "|  1.0| [1.0,2.0,3.0]|[-18.070405604444...|[1.41945802370898...|       1.0|\n",
      "|  1.0| [2.0,3.0,4.0]|[-38.987081234650...|[1.16983808020820...|       1.0|\n",
      "|  0.0|[-1.0,1.0,2.0]|[19.2085506510249...|[0.99999999545187...|       0.0|\n",
      "|  0.0|[-2.0,3.0,5.0]|[29.1902958840810...|[0.99999999999978...|       0.0|\n",
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lrModel = lr.fit(df)\n",
    "lrModel.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lrModel.save(\"lrModel.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "|label|      features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "|  1.0| [1.0,2.0,3.0]|[-18.070405604444...|[1.41945802370898...|       1.0|\n",
      "|  1.0| [2.0,3.0,4.0]|[-38.987081234650...|[1.16983808020820...|       1.0|\n",
      "|  0.0|[-1.0,1.0,2.0]|[19.2085506510249...|[0.99999999545187...|       0.0|\n",
      "|  0.0|[-2.0,3.0,5.0]|[29.1902958840810...|[0.99999999999978...|       0.0|\n",
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "\n",
    "sameModel = LogisticRegressionModel.load(\"lrModel.parquet\")\n",
    "sameModel.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"class\":\"org.apache.spark.ml.classification.LogisticRegressionModel\",\"timestamp\":1486475028230,\"sparkVersion\":\"2.1.0\",\"uid\":\"LogisticRegression_4846b56f70bf3abd7e5a\",\"paramMap\":{\"predictionCol\":\"prediction\",\"labelCol\":\"label\",\"tol\":1.0E-6,\"fitIntercept\":true,\"probabilityCol\":\"probability\",\"maxIter\":100,\"regParam\":0.0,\"standardization\":true,\"family\":\"auto\",\"featuresCol\":\"features\",\"threshold\":0.5,\"elasticNetParam\":0.0,\"rawPredictionCol\":\"rawPrediction\",\"aggregationDepth\":2}}\r\n"
     ]
    }
   ],
   "source": [
    "!cat lrModel.parquet/metadata/part-00000 "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
