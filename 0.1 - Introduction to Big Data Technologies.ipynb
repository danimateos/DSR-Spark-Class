{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big data technologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Big Data?\n",
    "\n",
    "*Big data is a term that describes the large volume of data – both structured and unstructured – that inundates a business on a day-to-day basis.* [SAS]\n",
    "\n",
    "*Big data is a collection of data from traditional and digital sources inside and outside your company that represents a source for ongoing discovery and analysis.* [Forbes]\n",
    "\n",
    "*Big data is high-volume, high-velocity and/or high-variety information assets that demand cost-effective, innovative forms of information processing that enable enhanced insight, decision making, and process automation.* [Gartner]\n",
    "\n",
    "*Big data is a term for data sets that are so large or complex that traditional data processing applications are inadequate to deal with them.* [Wikipedia.org]\n",
    "\n",
    "[Gartner]: http://www.gartner.com/it-glossary/big-data/\n",
    "[Forbes]: http://www.forbes.com/sites/lisaarthur/2013/08/15/what-is-big-data\n",
    "[SAS]: http://www.sas.com/en_us/insights/big-data/what-is-big-data.html\n",
    "[Wikipedia.org]: https://en.wikipedia.org/wiki/Big_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why big data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use cases\n",
    "\n",
    "#### \"*360 degree customer view*\"\n",
    "\n",
    "#### Reporting\n",
    "\n",
    "#### Fraud detection\n",
    "\n",
    "#### Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why now?\n",
    "\n",
    "### Big data is necessary\n",
    "\n",
    "Explosion of Internet-produced data: \n",
    "\n",
    "*As of 2014 Google has indexed 200 Terabytes (TB) of data (...) [which] is just an estimated 0.004 percent of the total Internet.* [quote]\n",
    "\n",
    "<!---\n",
    "Google faced this problem earlier, but every company faces it at some point\n",
    "--->\n",
    "\n",
    "[quote]: http://www.websitemagazine.com/content/blogs/posts/archive/2014/07/22/do-you-know-how-big-the-internet-really-is-infographic.aspx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The limits of vertical versus horizontal scaling\n",
    "\n",
    "There's a limit to how powerful a single machine can be, and to how much you can pay for it!\n",
    "\n",
    "![IBM z13s](http://www-03.ibm.com/systems/resources/z13s_overview_IntroProduct.jpg)\n",
    "\n",
    "<!---\n",
    "This is an entry-level machine. No one buys it because it's too weak. It costs $75,000\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---\n",
    "Anecdote time\n",
    "\n",
    "The expense and difficulty to upgrade\n",
    "--->\n",
    "\n",
    "![IBM z10](https://upload.wikimedia.org/wikipedia/commons/thumb/1/19/IBM_System_z10.jpg/180px-IBM_System_z10.jpg)\n",
    "\n",
    "More seriously, it's about cost-effectiveness: it does not make sense to pay for capabilities you don't need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big data is possible:\n",
    "\n",
    "![Hadoop Logo](https://upload.wikimedia.org/wikipedia/commons/thumb/0/0e/Hadoop_logo.svg/220px-Hadoop_logo.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Hadoop\n",
    "\n",
    "It is an open source software framework written in Java for **distributed** storage and processing of **very large data sets** on computer clusters built from **commodity hardware**. All the modules in Hadoop are designed with a fundamental assumption that hardware failures (of individual machines, or racks of machines) are common and thus should be automatically handled in software by the framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apache Hadoop and MapReduce: what is what\n",
    "\n",
    "Hadoop is a [software framework][hadoop github] with two main aspects:\n",
    "\n",
    "Distributed **storage** of data: HDFS\n",
    "\n",
    "and\n",
    "\n",
    "Distributed **processing** of data: MapReduce\n",
    "\n",
    "[hadoop github]: https://github.com/apache/hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Big data is possible: the origin\n",
    "\n",
    "\n",
    "#### The Google File System\n",
    "\"*The largest cluster to date provides **hundreds of terabytes of storage** across thousands of disks on over a thousand machines, and it is concurrently accessed by hundreds of clients.*\"\n",
    "\n",
    "([Ghemawat *et al.* 2003][GFS paper])\n",
    "\n",
    "#### MapReduce\n",
    "\"*...a typical MapReduce computation processes **many terabytes of data** on thousands of machines...*\"\n",
    "\n",
    "([Dean and Ghemawat, 2004][MapReduce paper])\n",
    "\n",
    "\n",
    "[GFS paper]: https://static.googleusercontent.com/media/research.google.com/es//archive/gfs-sosp2003.pdf\n",
    "[MapReduce paper]: https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2004!!\n",
    "\n",
    "![2004](https://www.mobilegazette.com/images/nokia-ngage-qd-5.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best processor of 2004:\n",
    "\n",
    "![Athlon 64 3500+](http://techreport.com/r.x/athlon64-3500/top-2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop Components\n",
    "\n",
    "### Storage: HDFS\n",
    "\n",
    "Assumptions in [HDFS design]:\n",
    "\n",
    "* The system is built from many inexpensive commodity components that often fail. \n",
    " <!---\n",
    "It must constantly monitor\n",
    "itself and detect, tolerate, and recover promptly from\n",
    "component failures on a routine basis.\n",
    "--->\n",
    "\n",
    "* The system stores a modest number of large files. \n",
    "<!---\n",
    "We expect a few million files, each typically 100 MB or\n",
    "larger in size. Multi-GB files are the common case\n",
    "and should be managed efficiently. Small files must be\n",
    "supported, but we need not optimize for them.\n",
    "--->\n",
    "\n",
    "* The workloads primarily consist of two kinds of reads: large streaming reads and small random reads. \n",
    "<!---\n",
    "In large streaming reads, individual operations typically\n",
    "read hundreds of KBs, more commonly 1 MB or more.\n",
    "Successive operations from the same client often read\n",
    "through a contiguous region of a file. A small random\n",
    "read typically reads a few KBs at some arbitrary\n",
    "offset. Performance-conscious applications often batch\n",
    "and sort their small reads to advance steadily through\n",
    "the file rather than go backand forth.\n",
    "--->\n",
    "\n",
    "* The workloads also have many large, sequential writes that append data to files.\n",
    "<!---\n",
    "Typical operation sizes are\n",
    "similar to those for reads. Once written, files are seldom\n",
    "modified again. Small writes at arbitrary positions\n",
    "in a file are supported but do not have to be\n",
    "efficient.\n",
    "--->\n",
    "\n",
    "* The system must efficiently implement well-defined semantics for multiple clients that concurrently append to the same file.\n",
    "<!---\n",
    "Our files are often used as producerconsumer\n",
    "queues or for many-way merging. Hundreds\n",
    "of producers, running one per machine, will concurrently\n",
    "append to a file. Atomicity with minimal synchronization\n",
    "overhead is essential. The file may be\n",
    "read later, or a consumer may be reading through the\n",
    "file simultaneously.\n",
    "--->\n",
    "\n",
    "* High sustained bandwidth is more important than low latency. \n",
    "<!---\n",
    "Most of our target applications place a premium\n",
    "on processing data in bulkat a high rate, while\n",
    "few have stringent response time requirements for an\n",
    "individual read or write.\n",
    "--->\n",
    "\n",
    "\n",
    "![HDFS](https://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/images/hdfsarchitecture.png)\n",
    "\n",
    "[HDFS design]: https://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop Components\n",
    "\n",
    "### Computation: MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---\n",
    "Comment that many many computations can be expressed \n",
    "in terms of maps and reduces and that we will see in\n",
    "detail how this works and even implement it in practice.\n",
    "--->\n",
    "\n",
    "![caption](http://d152j5tfobgaot.cloudfront.net/wp-content/uploads/2012/07/mapreduce.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "\n",
    "The three main problems that the MapReduce paper solved are ([1]):\n",
    "\n",
    "1. Parallelization — how to parallelize the computation\n",
    "2. Distribution — how to distribute the data\n",
    "3. Fault-tolerance — how to handle component failure\n",
    "\n",
    "[1]: https://medium.com/@markobonaci/the-history-of-hadoop-68984a11704#.xrcti0ney\n",
    "\n",
    "Hadoop allows distributed data storage and processing\n",
    "\n",
    "* Huge datasets\n",
    "\n",
    "* Clusters of off-the-shelf, cheap computers are more flexible and cost-effective than powerful bespoke machines.\n",
    "\n",
    "* When dealing with such a cluster, hardware failure is the norm, not the exception.\n",
    "\n",
    "* Moving computation is cheaper than moving data.\n",
    "\n",
    "* Vertical versus horizontal scaling.\n",
    "\n",
    "* Fault tolerance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce\n",
    "\n",
    "To thoroughly explain what is MapReduce and why it is called like that, we are going to have to make a detour through the *lambda calculus* and *functional programming*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Functional Programming]\n",
    "\n",
    "It is an alternative, long neglected, programming paradigm. That is, an alternative to imperative or object oriented programming.\n",
    "\n",
    "<!---\n",
    "These days you hear talk about functional programming like it's the second coming of Jesus. It seems like it's the newest fad, but it's actually quite old: The first famous functional language is Lisp, which was developed in the late 1950s- It's actually more than 10 years older than C!\n",
    "\n",
    "Funnily enough, functional programming and especially Lisp's rise and fall in popularity closely tracks the AI boom and subsequent winter in the 80s.\n",
    "\n",
    "[Lisp machine]\n",
    "\n",
    "--->\n",
    "\n",
    "Its fundamental building block is the function; any programming language that supports functional programming must have functions that are *first-class objects*. This means they can be passed around as arguments to other functions, modified, and returned.\n",
    "\n",
    "[functional programming]: https://en.wikipedia.org/wiki/Functional_programming\n",
    "[Lisp machine]: https://upload.wikimedia.org/wikipedia/commons/thumb/1/16/LISP_machine.jpg/330px-LISP_machine.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Lambda calculus]\n",
    "\n",
    "The Lambda Calculus is a formal system in mathematical logic for expressing computation based on function abstraction and application using variable binding and substitution. \n",
    "\n",
    "It is Turing-complete and consists of:\n",
    "\n",
    "* Function definition (declaration of expressions)\n",
    "* Function application (evaluation of those expressions)\n",
    "* Recursion (iteration)\n",
    "\n",
    "[Lambda calculus]: https://en.wikipedia.org/wiki/Lambda_calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher order functions\n",
    "\n",
    "HOFs are functions that take other functions as arguments, or that return other functions. The elemental HOFs are `map`, `reduce` and `filter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map: transform a collection applying a function element by element\n",
    "# https://docs.python.org/2/library/functions.html#map\n",
    "\n",
    "xs = range(1,10)\n",
    "\n",
    "map(lambda x: x + 1, xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16, 25, 36, 49, 64, 81]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise: generate from xs a list with the square of all numbers from 1 to 10\n",
    "\n",
    "map(lambda x: x ** 2, xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map\n",
    "\n",
    "![Map](https://cosminpupaza.files.wordpress.com/2015/10/map.png?w=505)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter: apply a boolean function to a collection element by element\n",
    "# keep those elements that return true, discard those that return false\n",
    "# https://docs.python.org/2/library/functions.html#filter\n",
    "\n",
    "filter(lambda x: x==2, xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise: get only the even numbers from xs\n",
    "\n",
    "filter(lambda x: x % 2 == 0, xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter\n",
    "![Filter](https://cosminpupaza.files.wordpress.com/2015/11/filter.png?w=405)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce: Apply function of two arguments cumulatively to the items of iterable,\n",
    "# from left to right, so as to reduce the iterable to a single value.\n",
    "# https://docs.python.org/2/library/functions.html#reduce\n",
    "\n",
    "reduce(lambda x,y: x + y, xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduce\n",
    "\n",
    "![Reduce](https://cosminpupaza.files.wordpress.com/2015/11/reduce.png?w=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n",
      "-43\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# We can optionally provide a starting value. This is usually called a fold.\n",
    "# The function that we have to give reduce has to be associative and commutative.\n",
    "# Can you argue why this would be important in a distributed environment?\n",
    "\n",
    "print reduce(lambda x, y: x + y, xs, 80)\n",
    "print reduce(lambda x, y: x - y, xs)\n",
    "print reduce(lambda x, y: y - x, xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Exercise: implement the max() function in terms of reduce()\n",
    "\n",
    "import random\n",
    "\n",
    "def maximum(collection):\n",
    "    return reduce(lambda x, y: x if x > y else y, collection)\n",
    "\n",
    "random_collection = [int(1000*random.random()) for i in range(10000)]\n",
    "\n",
    "assert maximum(random_collection) == max(random_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise: Calculate the mean of a collection of real numbers using map/reduce\n",
    "\n",
    "Recall:\n",
    "\n",
    "$$\\bar x = \\frac{\\sum_{i=1}^{N} x_i}{N} $$\n",
    "\n",
    "It´s straightforward to do this with the python methods sum() and len(). However, how would you do that with map/reduce? We have already seen how to sum the elements of a collection. Therefore we only need to implement `len()` in terms of map/reduce.\n",
    "\n",
    " * Create another array of the same size, consisting of 1s.\n",
    " * Sum the elements of that array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mean calculation: we need the sum and the number of elements\n",
    "\n",
    "import random\n",
    "\n",
    "random_collection = [int(1000*random.random()) for i in range(10000)]\n",
    "\n",
    "tuples = map(lambda x: 1, random_collection)\n",
    "count = reduce(lambda x,y: x + y, tuples)\n",
    "mean = reduce(lambda x,y: x + y, random_collection) / count\n",
    "\n",
    "assert count == len(random_collection)\n",
    "assert mean == sum(random_collection) / len(random_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise: Calculate the standard deviation of a collection of real numbers\n",
    "Recall:\n",
    "\n",
    "$$\\sigma_x^2 = \\frac{\\sum_{i=1}^{N} (x_i-\\bar x)^2}{N}$$\n",
    "\n",
    "For this, use the *mean* and *count* variables from the previous exercise.\n",
    "\n",
    "How many times do we need to iterate over the collection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from numpy import std\n",
    "\n",
    "square_differences = map(lambda x: (x - mean) ** 2, random_collection)\n",
    "stdev = sqrt(reduce(lambda x, y: x + y, square_differences) / count)\n",
    "\n",
    "assert abs(stdev - std(random_collection)) < 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise: calculate the mean and standard deviation in only one pass over the collection\n",
    "\n",
    "When working with distributed systems, we often have to adapt our first intuitive approach to a processing pipeline in order to obtain a more efficient strategy.\n",
    "\n",
    "The most computationally expensive step of a distributed processing pipeline is *always* network transmission, followed by persistence to disk.\n",
    "\n",
    "$$\\sigma_x^2 = \\frac{\\sum_{i=1}^{N} (x_i-\\bar x)^2}{N} =\n",
    "\\frac{\\sum_{i=1}^{N}\\left(x_i^2+\\bar x ^2-2x_i\\bar x\\right)}{N} = \n",
    "\\frac{\\sum_{i=1}^{N}x_i^2}{N} - \\bar x^2$$\n",
    "\n",
    "\n",
    "For the std calculation, we have obtained separatedly the sum of elements, the length and the sum of the elements squared. That is, we have swept the array three times! Can you do it in a two step process using map/reduce? Do you think it might matter at some point?\n",
    "\n",
    "* Hint: recall that reduce takes two arguments of the same type, and returns another value of that type. So, instead of using numbers as the elements of our collection, use tuples!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tuples = map(lambda n: (n, n ** 2, 1), random_collection)\n",
    "  \n",
    "    \n",
    "total_sum, sum_squares, count = reduce(lambda t1, t2: (t1[0] + t2[0], t1[1] + t2[1], t1[2] + t2[2]), tuples)\n",
    "\n",
    "mean = 1.0 * total_sum / count\n",
    "\n",
    "stdev = sqrt(sum_squares / count -  mean ** 2)\n",
    "\n",
    "assert abs(stdev - std(random_collection)) < 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The 'word-count' problem: creating histograms\n",
    "Given a set of keys (e.g. words) in an input collection, calculate the frequency of each key (word). \n",
    "\n",
    "In order to understand better how map/reduce works, we will implement this simple calculation in several forms.\n",
    "\n",
    "For simplicity, we are going to create a list of numbers between 1 and 9, that can be repated a (random) number of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 9, 5, 2, 6, 7, 2, 5, 6, 1, 2, 6, 4, 8, 6, 5, 9, 3, 9, 8, 8, 9, 8, 3, 7, 2, 5, 9, 3, 8, 8, 9, 1, 8, 4, 3, 5, 2, 4, 2, 4, 1, 5, 2, 5, 9, 7, 4, 2, 6, 5, 5, 1, 1, 3, 3, 1, 6, 6, 1, 2, 2, 1, 5, 7, 4, 3, 2, 1, 3, 2, 6, 3, 4, 3, 1, 8, 6, 9, 2, 8, 3, 5, 1, 4, 1, 9, 3, 2, 9, 7, 9, 6, 9, 5, 9, 2, 2, 7]\n"
     ]
    }
   ],
   "source": [
    "# Create collection of numbers with repeated elements\n",
    "\n",
    "import random\n",
    "random_collection = [random.randint(1,9) for x in range(1, 100)]\n",
    "\n",
    "print random_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive approach; no map/reduce\n",
    "\n",
    " * Start with an empty dict\n",
    " * If a new key is not present in the dict, create it.\n",
    " * Otherwise, increase the frequency of the key by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 13, 2: 16, 3: 12, 4: 8, 5: 12, 6: 10, 7: 6, 8: 9, 9: 13}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def histogram(collection):\n",
    "    # create empty dictionary\n",
    "    hist = {}\n",
    "    \n",
    "    for key in collection:\n",
    "        if key in hist: hist[key] +=1 \n",
    "        else: hist[key] = 1\n",
    "    return hist\n",
    "\n",
    "histogram(random_collection)\n",
    "\n",
    "# What would happen if the collection was too big to fit in memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map/reduce\n",
    "\n",
    " * Recall that *reduce* applies an operation to 2 elements of the same type, and returns another element of that type. Thus, first thing to do is to map our collection to the type of the output. We cannot use dicts, as dict(list) removes duplictaed keys. We will use list of tuples instead.\n",
    " * Then, we have to define a mehtod in the reducer that combines keys. There are two steps:\n",
    "   * Obtain the keys in the left list\n",
    "   * Then, check that the key in the second list already exists in the first one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 13), (9, 13), (5, 12), (2, 16), (6, 10), (7, 6), (4, 8), (8, 9), (3, 12)]\n"
     ]
    }
   ],
   "source": [
    "# Same histogram with map/reduce\n",
    "def histMR(l,verbose=False):\n",
    "    # emit an array of list of tuples\n",
    "    m = map(lambda i: [(i,1)], l) \n",
    "    return reduce(lambda i,j: combineKeys(i,j,verbose), m)\n",
    "\n",
    "def combineKeys(l1,l2,verbose=False):\n",
    "    '''\n",
    "    method to combine keys\n",
    "    \n",
    "    inputs: \n",
    "        * l1: list of tuples [(k1,v1),(k2,v2),...]        \n",
    "        * l2: list of single tuple [(kk,vv)]\n",
    "    \n",
    "    output: list of tuples [(k1,f1),(k2,f2),...].\n",
    "    '''\n",
    "    \n",
    "    # It is useful to print the process\n",
    "    if verbose:\n",
    "        print \"reducing\"\n",
    "        print l1\n",
    "        print l2\n",
    "        \n",
    "    # keys in left list\n",
    "    keys1 = map(lambda (key,value): key,l1)\n",
    "    # key in right list\n",
    "    k2 = l2[0][0]\n",
    "    if k2 in keys1:\n",
    "        # get the index of the key=k2 in keys1\n",
    "        index = keys1.index(k2)\n",
    "        # increase the value of that key accordingly\n",
    "        l1[index] = (k2,l1[index][1]+l2[0][1])\n",
    "    else:\n",
    "        # append the missing (key, value) pair to the left list\n",
    "        l1 = l1+l2\n",
    "    return l1\n",
    "\n",
    "# if you want to see how the reducer works, call histMR with verbose=True, i.e. histMR(a,True)\n",
    "print histMR(random_collection)\n",
    "\n",
    "assert histogram(random_collection) == dict(histMR(random_collection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the difference with the previous method, based on dictionaries: now, keys are not sorted!!\n",
    "\n",
    "But, where did we sorted the keys in `histogram()`? Well, we didn´t, but python's `dict` does that internally for us to speed up things. See the difference in time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 13), (9, 13), (5, 12), (2, 16), (6, 10), (7, 6), (4, 8), (8, 9), (3, 12)]\n",
      "{1: 13, 2: 16, 3: 12, 4: 8, 5: 12, 6: 10, 7: 6, 8: 9, 9: 13}\n",
      "100000 loops, best of 3: 11 µs per loop\n",
      "1000 loops, best of 3: 264 µs per loop\n"
     ]
    }
   ],
   "source": [
    "print histMR(random_collection)\n",
    "print dict(histMR(random_collection))\n",
    "%timeit histogram(random_collection)\n",
    "%timeit histMR(random_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Spark. Work with RDD and Pair RDD abstractions \n",
    "\n",
    "![pyspark](https://prateekvjoshi.files.wordpress.com/2015/10/1-main4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark\n",
    "\n",
    "Apache Spark is an open source cluster computing framework originally developed in the AMPLab at University of California, Berkeley but was later donated to the Apache Software Foundation where it remains today. In contrast to Hadoop's two-stage disk-based MapReduce paradigm, Spark's multi-stage in-memory primitives provides performance up to 100 times faster for certain applications.\n",
    "\n",
    "![Spark computing](http://image.slidesharecdn.com/sparkandshark-120620130508-phpapp01/95/spark-and-shark-8-728.jpg?cb=1340197567)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Spark?\n",
    "\n",
    "### Speed, especially when accessing repeatedly the same data:\n",
    "\n",
    "By allowing user programs to load data into a cluster's memory and query it repeatedly, Spark is well-suited to machine learning algorithms.\n",
    "\n",
    "![Logistic Regression](http://spark.apache.org/images/logistic-regression.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark comes with a number of components that provide flexibility and generality.\n",
    "\n",
    "![asdf](http://spark.apache.org/images/spark-stack.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Spark?\n",
    "\n",
    "### Simplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word count in [Hadoop][Hadoop word count]:\n",
    "\n",
    "```java\n",
    "import java.io.IOException;\n",
    "import java.util.StringTokenizer;\n",
    "\n",
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.fs.Path;\n",
    "import org.apache.hadoop.io.IntWritable;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Job;\n",
    "import org.apache.hadoop.mapreduce.Mapper;\n",
    "import org.apache.hadoop.mapreduce.Reducer;\n",
    "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n",
    "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n",
    "\n",
    "public class WordCount {\n",
    "\n",
    "  public static class TokenizerMapper\n",
    "       extends Mapper<Object, Text, Text, IntWritable>{\n",
    "\n",
    "    private final static IntWritable one = new IntWritable(1);\n",
    "    private Text word = new Text();\n",
    "\n",
    "    public void map(Object key, Text value, Context context\n",
    "                    ) throws IOException, InterruptedException {\n",
    "      StringTokenizer itr = new StringTokenizer(value.toString());\n",
    "      while (itr.hasMoreTokens()) {\n",
    "        word.set(itr.nextToken());\n",
    "        context.write(word, one);\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  public static class IntSumReducer\n",
    "       extends Reducer<Text,IntWritable,Text,IntWritable> {\n",
    "    private IntWritable result = new IntWritable();\n",
    "\n",
    "    public void reduce(Text key, Iterable<IntWritable> values,\n",
    "                       Context context\n",
    "                       ) throws IOException, InterruptedException {\n",
    "      int sum = 0;\n",
    "      for (IntWritable val : values) {\n",
    "        sum += val.get();\n",
    "      }\n",
    "      result.set(sum);\n",
    "      context.write(key, result);\n",
    "    }\n",
    "  }\n",
    "\n",
    "  public static void main(String[] args) throws Exception {\n",
    "    Configuration conf = new Configuration();\n",
    "    Job job = Job.getInstance(conf, \"word count\");\n",
    "    job.setJarByClass(WordCount.class);\n",
    "    job.setMapperClass(TokenizerMapper.class);\n",
    "    job.setCombinerClass(IntSumReducer.class);\n",
    "    job.setReducerClass(IntSumReducer.class);\n",
    "    job.setOutputKeyClass(Text.class);\n",
    "    job.setOutputValueClass(IntWritable.class);\n",
    "    FileInputFormat.addInputPath(job, new Path(args[0]));\n",
    "    FileOutputFormat.setOutputPath(job, new Path(args[1]));\n",
    "    System.exit(job.waitForCompletion(true) ? 0 : 1);\n",
    "  }\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "[Hadoop word count]: https://hadoop.apache.org/docs/r2.7.3/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word count in [Spark][word count in Spark]:\n",
    "\n",
    "\n",
    "```python\n",
    "text_file = sc.textFile(\"hdfs://...\")\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "counts.saveAsTextFile(\"hdfs://...\")\n",
    "```\n",
    "\n",
    "\n",
    "[word count in Spark]: http://spark.apache.org/examples.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## WordCount with Spark\n",
    "\n",
    "### The basic abstraction of Spark is the Resilient Distributed Dataset (RDD):\n",
    "\n",
    "*RDDs are fault-tolerant, parallel data structures that let users explicitly persist intermediate results in memory, control their partitioning to optimize data placement, and manipulate them using a rich set of operators.*\n",
    "\n",
    " * Read only, partitioned collection of records (immutable).\n",
    " * Stores the transformations used to build a dataset (its linage), instead of the data itself. This property ensures fault-tolerance.\n",
    " * User can control partitioning and persistence (caching).\n",
    " * RDDs are statically typed.\n",
    " * Native language is Scala\n",
    " \n",
    "<img src=\"http://eng.trueaccord.com/wp-content/uploads/2014/10/scala-logo.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "\n",
    "### We will be trying to understand this abstraction with simple examples, using the [Python API](http://spark.apache.org/docs/latest/api/python/index.html)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a base RDD: parallelize, actions and transformations\n",
    "\n",
    "We'll start by generating a base RDD by using a Python list and the `sc.parallelize` method.  Then we'll print out the type of the base RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:475\n"
     ]
    }
   ],
   "source": [
    "wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']\n",
    "wordsRDD = sc.parallelize(wordsList)\n",
    "# Print out the type of wordsRDD\n",
    "\n",
    "print wordsRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nothing has actually happened!**\n",
    "\n",
    "`parallellize` tells spark to distribute the data, but this is not actually done until we perform some action.\n",
    "\n",
    "Possible actions include couting, collecting, reducing, taking, etc. Take a look at http://spark.apache.org/docs/latest/programming-guide.html#actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'elephant', 'rat', 'rat', 'cat']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at what is going on in the shell. Have a look also at localhost:4040\n",
    "wordsRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'elephant']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsRDD.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[3] at RDD at PythonRDD.scala:48\n"
     ]
    }
   ],
   "source": [
    "# Apply a lambda function to our RDD to get the plural of each word\n",
    "\n",
    "pluralRDD = wordsRDD.map(lambda s: s+'s')\n",
    "print pluralRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cats', 'elephants', 'rats', 'rats', 'cats']\n"
     ]
    }
   ],
   "source": [
    "print pluralRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions we can apply are only limited by our imagination. For instance, we can obtain the length of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 9, 4, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "pluralLengths = (pluralRDD\n",
    "                 .map(lambda w: len(w))\n",
    "                 .collect())\n",
    "print pluralLengths\n",
    "assert pluralLengths == [4,9,4,4,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persisting and the RDD lineage\n",
    "\n",
    "So far, we have seen that Spark RDDs are *lazy evaluated*, i.e. nothing is actually done until an action is performed. In the RDD, the set of transformations to be applied are remembered: this is known as its *lineage*. It has the important consequence of making Spark RDDs *fault tolerant* automatically.\n",
    "\n",
    "It also allows Spark to optimize the execution of our queries.\n",
    "\n",
    "In technical terms, an RDD is what's known as a directed acyclic graph (DAG) that describes the computation. You can find more details [here][stack overflow Spark DAG]. \n",
    "\n",
    "![Lineage](http://images.slideplayer.com/14/4499833/slides/slide_10.jpg) \n",
    "\n",
    "It might be interesting to store some intermediate results, though: perhaps because we want to apply several different transformations starting from that point, or because we are going to apply an iterative computation (as is customary in machine learning algorithms). For this, Spark has [several ways of persisting]\n",
    "\n",
    "[several ways of persisting]: (http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence)\n",
    "\n",
    "[stack overflow Spark DAG]: http://stackoverflow.com/questions/25836316/how-dag-works-under-the-covers-in-rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Memory Serialized 1x Replicated\n"
     ]
    }
   ],
   "source": [
    "# note that the RDD is not cached until the first action is performed!! \n",
    "# Take a look at http://localhost:4040/storage/\n",
    "wordsCached = wordsRDD.cache()\n",
    "print wordsCached.is_cached\n",
    "print wordsCached.getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cats', 'elephants', 'rats', 'rats', 'cats']\n",
      "['cat is an animal', 'elephant is an animal', 'rat is an animal', 'rat is an animal', 'cat is an animal']\n"
     ]
    }
   ],
   "source": [
    "print wordsCached.map(lambda s: s + 's').collect()\n",
    "print wordsCached.map(lambda s: s + ' is an animal').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Serialized 1x Replicated\n"
     ]
    }
   ],
   "source": [
    "# default persisting\n",
    "wordsCached.unpersist()\n",
    "wordsCached.persist(StorageLevel.MEMORY_ONLY)\n",
    "wordsCached.collect()\n",
    "print wordsCached.getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Serialized 1x Replicated\n"
     ]
    }
   ],
   "source": [
    "# play with other levels of storage, and see how it changes in http://localhost:4040/storage/\n",
    "wordsCached.unpersist()\n",
    "wordsCached.persist(StorageLevel.MEMORY_ONLY_SER)\n",
    "wordsCached.collect()\n",
    "print wordsCached.getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:475"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsCached.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning\n",
    "\n",
    "One important parameter for parallel collections is the number of partitions to cut the dataset into. Spark will run one task for each partition of the cluster. Typically you want 2-4 partitions for each CPU in your cluster.\n",
    "\n",
    "To get the number of partitions of an RDD, just use `getNumPartitions()` on your RDD. You can change the partitions during RDD creation (with `parallelize(collection,numPartitions)` or `fromTextFile(file,numPartitions)`), or afterwards with methos like `repartition(), coalesce()`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rat', 'cat', 'cat', 'elephant', 'rat']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change the number of partitions\n",
    "wordRepartition = wordsRDD.repartition(5)\n",
    "wordRepartition.getNumPartitions()\n",
    "\n",
    "# in order to see the efects in the browser, we cache an apply an action\n",
    "wordRepartition.cache().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the partitions using [glom()](http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=glom#pyspark.RDD.glom): it returns an RDD created by coalescing all elements within each partition into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['rat', 'cat'], ['cat', 'elephant'], [], [], ['rat']]\n",
      "[['cat'], ['elephant'], ['rat'], ['rat', 'cat']]\n",
      "[['cat', 'elephant'], ['rat', 'rat', 'cat']]\n"
     ]
    }
   ],
   "source": [
    "print wordRepartition.glom().collect()\n",
    "print wordsRDD.glom().collect()\n",
    "print wordsRDD.coalesce(2).glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partitions are one of the most powerfull concepts in Spark: you can decide how to distribute your data so it can fit in memory, and more importantly, you can perform computations on each partition *before* speaking to other partitions. This can have an enorumous impact on performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair RDDs: *grouping* strategies in Spark\n",
    "\n",
    "The next step in writing our word counting program is to create a new type of RDD, called a pair RDD. A pair RDD is an RDD where each element is a pair tuple (k, v) where k is the key and v is the value. In this example, we will create a pair consisting of (`word`, 1) for each word element in the RDD, as we did in the map/reduce version of the histogram in Python, section (1d.2).\n",
    "\n",
    "We can create the pair RDD using the map() transformation with a lambda() function to create a new RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 1), ('elephant', 1), ('rat', 1), ('rat', 1), ('cat', 1)]\n"
     ]
    }
   ],
   "source": [
    "wordPairs = wordsRDD.map(lambda w: (w,1))\n",
    "print wordPairs.collect()\n",
    "assert wordPairs.collect() == [('cat', 1), ('elephant', 1), ('rat', 1), ('rat', 1), ('cat', 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `groupByKey()` approach\n",
    "An approach you might first consider (we'll see shortly that there are better ways) is based on using the [groupByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.groupByKey) transformation. As the name implies, the `groupByKey()` transformation groups all the elements of the RDD with the same key into a single list in one of the partitions. There are two problems with using `groupByKey()`:\n",
    "  + The operation requires a lot of data movement to move all the values into the appropriate partitions.\n",
    "  + The lists can be very large. Consider a word count of English Wikipedia: the lists for common words (e.g., the, a, etc.) would be huge and could exhaust the available memory in a worker.\n",
    " \n",
    "![](http://blog.cheyo.net/static/file/spark_groupByKey.jpg)\n",
    "\n",
    "Use `groupByKey()` to generate a pair RDD of type `('word', iterator)`. Next, sum the iterator using a `map()` transformation.  The result should be a pair RDD consisting of (word, count) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rat: [1, 1]\n",
      "elephant: [1]\n",
      "cat: [1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Note that groupByKey requires no parameters\n",
    "wordsGrouped = wordPairs.groupByKey()\n",
    "# Print the key and the values corresponding to that word\n",
    "for key, value in wordsGrouped.collect():\n",
    "    print '{0}: {1}'.format(key,list(value))\n",
    "    \n",
    "assert ( sorted(wordsGrouped.mapValues(lambda x: list(x)).collect()) ==\n",
    "        [('cat', [1, 1]), ('elephant', [1]), ('rat', [1, 1])] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rat', 2), ('elephant', 1), ('cat', 2)]\n"
     ]
    }
   ],
   "source": [
    "wordCountsGrouped = wordsGrouped.map(lambda (k,iterator): (k,sum(iterator)))\n",
    "# also we can mapValues directly:\n",
    "wordCountsGrouped = wordsGrouped.mapValues(lambda iterator: sum(iterator))\n",
    "print wordCountsGrouped.collect()\n",
    "\n",
    "assert sorted(wordCountsGrouped.collect()) == [('cat', 2), ('elephant', 1), ('rat', 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `reduceByKey` approach\n",
    "A better approach is to start from the pair RDD and then use the [reduceByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey) transformation to create a new pair RDD. \n",
    "\n",
    "The `reduceByKey()` transformation gathers together pairs that have the same key and applies the function provided to two values at a time, iteratively reducing all of the values to a single value. `reduceByKey()` operates by applying the function first within each partition on a per-key basis and then across the partitions, allowing it to scale efficiently to large datasets.\n",
    "\n",
    "![](https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/images/reduce_by.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rat', 2), ('elephant', 1), ('cat', 2)]\n",
      "[('rat', 2), ('elephant', 1), ('cat', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Note that reduceByKey takes in a function that accepts two values and returns a single value\n",
    "wordCounts = wordPairs.reduceByKey(lambda a,b:a+b)\n",
    "print wordCounts.collect()\n",
    "\n",
    "# with add operator\n",
    "from operator import add\n",
    "wordCountsMod = wordPairs.reduceByKey(add)\n",
    "print wordCountsMod.collect()\n",
    "\n",
    "assert sorted(wordCounts.collect()) == [('cat', 2), ('elephant', 1), ('rat', 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rat', 2), ('elephant', 1), ('cat', 2)]\n"
     ]
    }
   ],
   "source": [
    "# All together: create tuples of (word,1), and then apply the reduceByKey method\n",
    "# to obtain the frequency of each word:\n",
    "wordCountsCollected = (wordsRDD\n",
    "                       .map(lambda x: (x,1))\n",
    "                       .reduceByKey(add)\n",
    "                       .collect())\n",
    "print wordCountsCollected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: WordCount on actual data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a text file\n",
    "For the next part of this lab, we will use the [Complete Works of William Shakespeare](http://www.gutenberg.org/ebooks/100) from [Project Gutenberg](http://www.gutenberg.org/wiki/Main_Page). To convert a text file into an RDD, we use the `SparkContext.textFile()` method. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-01-20 16:48:47--  http://www.gutenberg.org/ebooks/100.txt.utf-8\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: http://www.gutenberg.org/cache/epub/100/pg100.txt [following]\n",
      "--2017-01-20 16:48:47--  http://www.gutenberg.org/cache/epub/100/pg100.txt\n",
      "Reusing existing connection to www.gutenberg.org:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5589889 (5,3M) [text/plain]\n",
      "Saving to: ‘shakespeare.txt’\n",
      "\n",
      "shakespeare.txt     100%[===================>]   5,33M   355KB/s    in 18s     \n",
      "\n",
      "2017-01-20 16:49:06 (304 KB/s) - ‘shakespeare.txt’ saved [5589889/5589889]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The complete works of William Shakespeare\n",
    "# We are using the Unix command wget; this won't work in\n",
    "# Windows computers.\n",
    "\n",
    "!wget -O shakespeare.txt http://www.gutenberg.org/ebooks/100.txt.utf-8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: The Project Gutenberg EBook of The Complete Works of William Shakespeare, by\n",
      "1: William Shakespeare\n",
      "2: \n",
      "3: This eBook is for the use of anyone anywhere at no cost and with\n",
      "4: almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "5: re-use it under the terms of the Project Gutenberg License included\n",
      "6: with this eBook or online at www.gutenberg.org\n",
      "7: \n",
      "8: ** This is a COPYRIGHTED Project Gutenberg eBook, Details Below **\n",
      "9: **     Please follow the copyright guidelines in this file.     **\n",
      "10: \n",
      "11: Title: The Complete Works of William Shakespeare\n",
      "12: \n",
      "13: Author: William Shakespeare\n",
      "14: \n"
     ]
    }
   ],
   "source": [
    "# Just run this code\n",
    "\n",
    "import os.path\n",
    "baseDir = os.path.join('../AUDI-SPARK-TRAINING') # wherever you have put the file 'shakespeare.txt'\n",
    "fileName = os.path.join(baseDir, 'shakespeare.txt')\n",
    "def format_tuple(tuple2):\n",
    "    return '{0}: {1}'.format(tuple2[1], tuple2[0])\n",
    "\n",
    "\n",
    "shakespeareRDD = sc.textFile(fileName, 8)\n",
    "\n",
    "print '\\n'.join(shakespeareRDD\n",
    "                .zipWithIndex()  # to (line, lineNum)\n",
    "                .map(format_tuple)  # to 'lineNum: line'\n",
    "                .take(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to disregard capitalization variants and punctuation, we need to define a `remove_punctuation()` function. It will take a single line and clean it up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function sub in module re:\n",
      "\n",
      "sub(pattern, repl, string, count=0, flags=0)\n",
      "    Return the string obtained by replacing the leftmost\n",
      "    non-overlapping occurrences of the pattern in string by the\n",
      "    replacement repl.  repl can be either a string or a callable;\n",
      "    if a string, backslash escapes in it are processed.  If it is\n",
      "    a callable, it's passed the match object and must return\n",
      "    a replacement string to be used.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "help(re.sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi you\n",
      "no underscore\n",
      "the elephants 4 cats\n"
     ]
    }
   ],
   "source": [
    "# Exercise: define the remove_punctuation function\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"Removes punctuation, changes to lower case, and strips leading and trailing spaces.\n",
    "\n",
    "    Note:\n",
    "        Only spaces, letters, and numbers should be retained.  Other characters should should be\n",
    "        eliminated (e.g. it's becomes its).  Leading and trailing spaces should be removed after\n",
    "        punctuation is removed.\n",
    "\n",
    "    Args:\n",
    "        text (str): A string.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned up string.\n",
    "    \"\"\"\n",
    "    return re.sub('[^a-zA-Z0-9 ]','',text.lower().strip())\n",
    "\n",
    "assert remove_punctuation(\" The Elephant's 4 cats. \") == 'the elephants 4 cats'\n",
    "print remove_punctuation('Hi, you!')\n",
    "print remove_punctuation(' No under_score!')\n",
    "print remove_punctuation(\" The Elephant's 4 cats. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a collection of lines (values) and a function that takes one value and outputs one value. \n",
    "\n",
    "What tool from our functional programming toolbox can we use to preprocess the complete works of Shakespeare in one line of code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'the project gutenberg ebook of the complete works of william shakespeare by',\n",
       " u'william shakespeare',\n",
       " u'',\n",
       " u'this ebook is for the use of anyone anywhere at no cost and with',\n",
       " u'almost no restrictions whatsoever  you may copy it give it away or',\n",
       " u'reuse it under the terms of the project gutenberg license included',\n",
       " u'with this ebook or online at wwwgutenbergorg',\n",
       " u'',\n",
       " u' this is a copyrighted project gutenberg ebook details below ',\n",
       " u'     please follow the copyright guidelines in this file     ',\n",
       " u'',\n",
       " u'title the complete works of william shakespeare',\n",
       " u'',\n",
       " u'author william shakespeare',\n",
       " u'']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_shakespeare = shakespeareRDD.map(remove_punctuation)\n",
    "\n",
    "clean_shakespeare.take(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words from lines\n",
    "\n",
    "Before we can proceed with word counting, we have to address two issues with the format of the RDD:\n",
    "  + The first issue is that  that we need to split each line by its spaces.\n",
    "  + The second issue is we need to filter out empty lines.\n",
    "  \n",
    "Apply a transformation that will split each element of the RDD by its spaces. For each element of the RDD, you should apply Python's string [`split()`] function. You might think that a `map()` transformation is the way to do this, but think about what the result of the `split()` function will be.\n",
    "\n",
    "[`split()`]: https://docs.python.org/2/library/string.html#string.split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "single_words = clean_shakespeare.flatMap(lambda line: line.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove empty elements\n",
    "\n",
    "The next step is to filter out the empty elements.  Remove all entries where the word is `''`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'the', u'project', u'gutenberg', u'ebook', u'of']\n",
      "903705\n"
     ]
    }
   ],
   "source": [
    "nonempty_words = single_words.filter(lambda word: word !='')\n",
    "number_words = nonempty_words.count()\n",
    "print nonempty_words.take(5)\n",
    "print number_words\n",
    "assert number_words == 903705"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the words\n",
    "\n",
    "We now have an RDD that is only words. We are at the point where the classic word count example starts- they never show the not-that-sexy parts in the examples, do they?\n",
    "\n",
    "Remember the classic example: it consists of applying a transformation (`map()`) and then an action (`reduce()`). What are the functions that we need to pass to `map` and to `reduce`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_counts = nonempty_words \\\n",
    "  .map(lambda w: (w, 1)) \\\n",
    "  .reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have an RDD of words and counts, we can use the `takeOrdered()` method of RDDs to obtain the fifteen most common words and their counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27825: the\n",
      "26791: and\n",
      "20681: i\n",
      "19261: to\n",
      "18289: of\n",
      "14667: a\n",
      "13716: you\n",
      "12481: my\n",
      "11135: that\n",
      "11027: in\n",
      "9621: is\n",
      "8745: not\n",
      "8261: for\n",
      "8046: with\n",
      "7769: me\n"
     ]
    }
   ],
   "source": [
    "top_15 = word_counts \\\n",
    "  .takeOrdered(15, key=lambda x: -x[1])\n",
    "    \n",
    "print '\\n'.join(map(format_tuple, top_15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that many of the words are common English words (know as stopwords)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
