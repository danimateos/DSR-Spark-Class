{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f4bc6b07710>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark ML Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Elements:\n",
    "- Model (Estimator or Pipeline)\n",
    "- Set of ParamMaps (to perform the grid search) - you should use the ***ParamGridBuilder*** utility\n",
    "- Evaluator (to assess the fitness of the model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Splits the dataset into K folds\n",
    "- Each fold is splitted into a training (2/3) and a test (1/3) sets\n",
    "- It will fit K models and compute the average of the K evaluation metrics (according to the Evaluator)\n",
    "- Based on the metrics, it will determine the best set of parameters\n",
    "- Then it will fit the model one final time, using this set of parameters and the whole dataset\n",
    "- This is a VERY computationally expensive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf metastore_db/*.lck\n",
    "\n",
    "training = sqlc.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0),\n",
    "    (4, \"b spark who\", 1.0),\n",
    "    (5, \"g d a y\", 0.0),\n",
    "    (6, \"spark fly\", 1.0),\n",
    "    (7, \"was mapreduce\", 0.0),\n",
    "    (8, \"e spark program\", 1.0),\n",
    "    (9, \"a e c l\", 0.0),\n",
    "    (10, \"spark compile\", 1.0),\n",
    "    (11, \"hadoop software\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{Param(parent=u'LogisticRegression_4a49a578ae4aae2ac322', name='regParam', doc='regularization parameter (>= 0).'): 0.1,\n",
       "  Param(parent=u'HashingTF_4b558295ebfb444a0fac', name='numFeatures', doc='number of features.'): 10},\n",
       " {Param(parent=u'LogisticRegression_4a49a578ae4aae2ac322', name='regParam', doc='regularization parameter (>= 0).'): 0.1,\n",
       "  Param(parent=u'HashingTF_4b558295ebfb444a0fac', name='numFeatures', doc='number of features.'): 100},\n",
       " {Param(parent=u'LogisticRegression_4a49a578ae4aae2ac322', name='regParam', doc='regularization parameter (>= 0).'): 0.1,\n",
       "  Param(parent=u'HashingTF_4b558295ebfb444a0fac', name='numFeatures', doc='number of features.'): 1000},\n",
       " {Param(parent=u'LogisticRegression_4a49a578ae4aae2ac322', name='regParam', doc='regularization parameter (>= 0).'): 0.01,\n",
       "  Param(parent=u'HashingTF_4b558295ebfb444a0fac', name='numFeatures', doc='number of features.'): 10},\n",
       " {Param(parent=u'LogisticRegression_4a49a578ae4aae2ac322', name='regParam', doc='regularization parameter (>= 0).'): 0.01,\n",
       "  Param(parent=u'HashingTF_4b558295ebfb444a0fac', name='numFeatures', doc='number of features.'): 100},\n",
       " {Param(parent=u'LogisticRegression_4a49a578ae4aae2ac322', name='regParam', doc='regularization parameter (>= 0).'): 0.01,\n",
       "  Param(parent=u'HashingTF_4b558295ebfb444a0fac', name='numFeatures', doc='number of features.'): 1000}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3)\n",
    "\n",
    "cvModel = crossval.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5833333333333333,\n",
       " 0.5833333333333333,\n",
       " 0.6111111111111112,\n",
       " 0.5833333333333333,\n",
       " 0.5833333333333333,\n",
       " 0.6111111111111112]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_453db7cd8407daaccfb0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tokenizer_4884a4a41b967001185a,\n",
       " HashingTF_4b558295ebfb444a0fac,\n",
       " LogisticRegression_4a49a578ae4aae2ac322]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.bestModel.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lr_best = cvModel.bestModel.stages[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(1000, {6: -1.4383, 66: -0.7317, 94: -0.4712, 105: 2.0311, 170: -0.1318, 181: -0.7851, 217: 0.9701, 234: -0.7317, 248: 0.755, 282: 0.755, 315: -0.7966, 361: 0.2486, 417: -0.0229, 463: 1.0866, 644: 0.8376, 695: 1.0866, 722: 0.2602, 878: 0.534, 953: -0.7851})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_best.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lr_summary = lr_best.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_summary.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "test = sqlc.createDataFrame([\n",
    "    (4L, \"spark i j k\"),\n",
    "    (5L, \"l m n\"),\n",
    "    (6L, \"mapreduce spark\"),\n",
    "    (7L, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id=4, text=u'spark i j k', probability=DenseVector([0.2661, 0.7339]), prediction=1.0)\n",
      "Row(id=5, text=u'l m n', probability=DenseVector([0.9209, 0.0791]), prediction=0.0)\n",
      "Row(id=6, text=u'mapreduce spark', probability=DenseVector([0.4429, 0.5571]), prediction=1.0)\n",
      "Row(id=7, text=u'apache hadoop', probability=DenseVector([0.8584, 0.1416]), prediction=0.0)\n"
     ]
    }
   ],
   "source": [
    "prediction = cvModel.transform(test)\n",
    "\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "\n",
    "for row in selected.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Train-Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It uses the entire dataset\n",
    "- The dataset is splitted into a training and a test sets according to the ***trainRatio*** parameter\n",
    "- It will fit a model for each set of parameters and evaluate its metrics (according to the Evaluator)\n",
    "- Based on the metrics, it will determine the best set of parameters\n",
    "- Then it will fit the model one final time, using this set of parameters and the whole dataset\n",
    "- This is a much less expensive, but it may not yield good results if the dataset is not large enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "data = sqlc.read.format(\"libsvm\").load(\"/home/ubuntu/spark/data/mllib/sample_linear_regression_data.txt\")\n",
    "\n",
    "train, test = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "lr = LinearRegression(maxIter=20, regParam=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{Param(parent=u'LinearRegression_4e67b8a0403fd4f2477e', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0,\n",
       "  Param(parent=u'LinearRegression_4e67b8a0403fd4f2477e', name='regParam', doc='regularization parameter (>= 0).'): 0.1},\n",
       " {Param(parent=u'LinearRegression_4e67b8a0403fd4f2477e', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0,\n",
       "  Param(parent=u'LinearRegression_4e67b8a0403fd4f2477e', name='regParam', doc='regularization parameter (>= 0).'): 0.01},\n",
       " {Param(parent=u'LinearRegression_4e67b8a0403fd4f2477e', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.5,\n",
       "  Param(parent=u'LinearRegression_4e67b8a0403fd4f2477e', name='regParam', doc='regularization parameter (>= 0).'): 0.1},\n",
       " {Param(parent=u'LinearRegression_4e67b8a0403fd4f2477e', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.5,\n",
       "  Param(parent=u'LinearRegression_4e67b8a0403fd4f2477e', name='regParam', doc='regularization parameter (>= 0).'): 0.01},\n",
       " {Param(parent=u'LinearRegression_4e67b8a0403fd4f2477e', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 1.0,\n",
       "  Param(parent=u'LinearRegression_4e67b8a0403fd4f2477e', name='regParam', doc='regularization parameter (>= 0).'): 0.1},\n",
       " {Param(parent=u'LinearRegression_4e67b8a0403fd4f2477e', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 1.0,\n",
       "  Param(parent=u'LinearRegression_4e67b8a0403fd4f2477e', name='regParam', doc='regularization parameter (>= 0).'): 0.01}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "\n",
    "tvs = TrainValidationSplit(estimator=lr,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=RegressionEvaluator(),\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "model = tvs.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression_4e67b8a0403fd4f2477e"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-0.7614, 1.3539, -0.0553, 2.7065, 0.2539, 1.5791, -0.0471, 0.6942, -0.8981, 0.9647])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bestModel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0513340249451959"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bestModel.summary.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "prediction = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>features</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-28.571479</td>\n",
       "      <td>(-0.45971844654, -0.548942938693, 0.3342291457...</td>\n",
       "      <td>-0.727912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-26.805483</td>\n",
       "      <td>(0.457255270422, -0.576096954, -0.20809839485,...</td>\n",
       "      <td>0.098400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-23.510884</td>\n",
       "      <td>(-0.468353842218, 0.146954018594, 0.9113612952...</td>\n",
       "      <td>0.504391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-23.487440</td>\n",
       "      <td>(-0.519535443126, 0.808035794841, 0.8498613208...</td>\n",
       "      <td>0.640791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-20.212077</td>\n",
       "      <td>(0.560906580841, -0.920190439115, 0.9083058651...</td>\n",
       "      <td>1.700844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                           features  prediction\n",
       "0 -28.571479  (-0.45971844654, -0.548942938693, 0.3342291457...   -0.727912\n",
       "1 -26.805483  (0.457255270422, -0.576096954, -0.20809839485,...    0.098400\n",
       "2 -23.510884  (-0.468353842218, 0.146954018594, 0.9113612952...    0.504391\n",
       "3 -23.487440  (-0.519535443126, 0.808035794841, 0.8498613208...    0.640791\n",
       "4 -20.212077  (0.560906580841, -0.920190439115, 0.9083058651...    1.700844"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.toPandas()[:5]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
